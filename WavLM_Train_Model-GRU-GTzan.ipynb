{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc547b1-84d4-4ba6-a8a1-8877423fe056",
   "metadata": {},
   "source": [
    "\n",
    "# Train GRU Model from WavLM features and discrete labels\n",
    "\n",
    "### For GTzan dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760109a1-c818-49cf-abd3-377b65994a33",
   "metadata": {},
   "source": [
    "##### https://github.com/microsoft/unilm/tree/master/wavlm\n",
    "##### https://github.com/audeering/w2v2-how-to/blob/main/notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8263d9b9-de2b-418e-b1a5-da5323499a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814ae24",
   "metadata": {},
   "source": [
    "### Process Feature Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82648fce-e07c-4188-a7b0-d1bd585c1e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/etsmtl/akoerich/DEV/Music\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191da55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blues.00000.wavlmbase4layerfeat',\n",
       " 'blues.00001.wavlmbase4layerfeat',\n",
       " 'blues.00002.wavlmbase4layerfeat',\n",
       " 'blues.00003.wavlmbase4layerfeat',\n",
       " 'blues.00004.wavlmbase4layerfeat',\n",
       " 'blues.00005.wavlmbase4layerfeat',\n",
       " 'blues.00006.wavlmbase4layerfeat',\n",
       " 'blues.00007.wavlmbase4layerfeat',\n",
       " 'blues.00008.wavlmbase4layerfeat',\n",
       " 'blues.00009.wavlmbase4layerfeat',\n",
       " 'blues.00010.wavlmbase4layerfeat',\n",
       " 'blues.00011.wavlmbase4layerfeat',\n",
       " 'blues.00012.wavlmbase4layerfeat',\n",
       " 'blues.00013.wavlmbase4layerfeat',\n",
       " 'blues.00014.wavlmbase4layerfeat',\n",
       " 'blues.00015.wavlmbase4layerfeat',\n",
       " 'blues.00016.wavlmbase4layerfeat',\n",
       " 'blues.00017.wavlmbase4layerfeat',\n",
       " 'blues.00018.wavlmbase4layerfeat',\n",
       " 'blues.00019.wavlmbase4layerfeat',\n",
       " 'blues.00020.wavlmbase4layerfeat',\n",
       " 'blues.00021.wavlmbase4layerfeat',\n",
       " 'blues.00022.wavlmbase4layerfeat',\n",
       " 'blues.00023.wavlmbase4layerfeat',\n",
       " 'blues.00024.wavlmbase4layerfeat',\n",
       " 'blues.00025.wavlmbase4layerfeat',\n",
       " 'blues.00026.wavlmbase4layerfeat',\n",
       " 'blues.00027.wavlmbase4layerfeat',\n",
       " 'blues.00028.wavlmbase4layerfeat',\n",
       " 'blues.00029.wavlmbase4layerfeat',\n",
       " 'blues.00030.wavlmbase4layerfeat',\n",
       " 'blues.00031.wavlmbase4layerfeat',\n",
       " 'blues.00032.wavlmbase4layerfeat',\n",
       " 'blues.00033.wavlmbase4layerfeat',\n",
       " 'blues.00034.wavlmbase4layerfeat',\n",
       " 'blues.00035.wavlmbase4layerfeat',\n",
       " 'blues.00036.wavlmbase4layerfeat',\n",
       " 'blues.00037.wavlmbase4layerfeat',\n",
       " 'blues.00038.wavlmbase4layerfeat',\n",
       " 'blues.00039.wavlmbase4layerfeat',\n",
       " 'blues.00040.wavlmbase4layerfeat',\n",
       " 'blues.00041.wavlmbase4layerfeat',\n",
       " 'blues.00042.wavlmbase4layerfeat',\n",
       " 'blues.00043.wavlmbase4layerfeat',\n",
       " 'blues.00044.wavlmbase4layerfeat',\n",
       " 'blues.00045.wavlmbase4layerfeat',\n",
       " 'blues.00046.wavlmbase4layerfeat',\n",
       " 'blues.00047.wavlmbase4layerfeat',\n",
       " 'blues.00048.wavlmbase4layerfeat',\n",
       " 'blues.00049.wavlmbase4layerfeat',\n",
       " 'blues.00050.wavlmbase4layerfeat',\n",
       " 'blues.00051.wavlmbase4layerfeat',\n",
       " 'blues.00052.wavlmbase4layerfeat',\n",
       " 'blues.00053.wavlmbase4layerfeat',\n",
       " 'blues.00054.wavlmbase4layerfeat',\n",
       " 'blues.00055.wavlmbase4layerfeat',\n",
       " 'blues.00056.wavlmbase4layerfeat',\n",
       " 'blues.00057.wavlmbase4layerfeat',\n",
       " 'blues.00058.wavlmbase4layerfeat',\n",
       " 'blues.00059.wavlmbase4layerfeat',\n",
       " 'blues.00060.wavlmbase4layerfeat',\n",
       " 'blues.00061.wavlmbase4layerfeat',\n",
       " 'blues.00062.wavlmbase4layerfeat',\n",
       " 'blues.00063.wavlmbase4layerfeat',\n",
       " 'blues.00064.wavlmbase4layerfeat',\n",
       " 'blues.00065.wavlmbase4layerfeat',\n",
       " 'blues.00066.wavlmbase4layerfeat',\n",
       " 'blues.00067.wavlmbase4layerfeat',\n",
       " 'blues.00068.wavlmbase4layerfeat',\n",
       " 'blues.00069.wavlmbase4layerfeat',\n",
       " 'blues.00070.wavlmbase4layerfeat',\n",
       " 'blues.00071.wavlmbase4layerfeat',\n",
       " 'blues.00072.wavlmbase4layerfeat',\n",
       " 'blues.00073.wavlmbase4layerfeat',\n",
       " 'blues.00074.wavlmbase4layerfeat',\n",
       " 'blues.00075.wavlmbase4layerfeat',\n",
       " 'blues.00076.wavlmbase4layerfeat',\n",
       " 'blues.00077.wavlmbase4layerfeat',\n",
       " 'blues.00078.wavlmbase4layerfeat',\n",
       " 'blues.00079.wavlmbase4layerfeat',\n",
       " 'blues.00080.wavlmbase4layerfeat',\n",
       " 'blues.00081.wavlmbase4layerfeat',\n",
       " 'blues.00082.wavlmbase4layerfeat',\n",
       " 'blues.00083.wavlmbase4layerfeat',\n",
       " 'blues.00084.wavlmbase4layerfeat',\n",
       " 'blues.00085.wavlmbase4layerfeat',\n",
       " 'blues.00086.wavlmbase4layerfeat',\n",
       " 'blues.00087.wavlmbase4layerfeat',\n",
       " 'blues.00088.wavlmbase4layerfeat',\n",
       " 'blues.00089.wavlmbase4layerfeat',\n",
       " 'blues.00090.wavlmbase4layerfeat',\n",
       " 'blues.00091.wavlmbase4layerfeat',\n",
       " 'blues.00092.wavlmbase4layerfeat',\n",
       " 'blues.00093.wavlmbase4layerfeat',\n",
       " 'blues.00094.wavlmbase4layerfeat',\n",
       " 'blues.00095.wavlmbase4layerfeat',\n",
       " 'blues.00096.wavlmbase4layerfeat',\n",
       " 'blues.00097.wavlmbase4layerfeat',\n",
       " 'blues.00098.wavlmbase4layerfeat',\n",
       " 'blues.00099.wavlmbase4layerfeat',\n",
       " 'classical.00000.wavlmbase4layerfeat',\n",
       " 'classical.00001.wavlmbase4layerfeat',\n",
       " 'classical.00002.wavlmbase4layerfeat',\n",
       " 'classical.00003.wavlmbase4layerfeat',\n",
       " 'classical.00004.wavlmbase4layerfeat',\n",
       " 'classical.00005.wavlmbase4layerfeat',\n",
       " 'classical.00006.wavlmbase4layerfeat',\n",
       " 'classical.00007.wavlmbase4layerfeat',\n",
       " 'classical.00008.wavlmbase4layerfeat',\n",
       " 'classical.00009.wavlmbase4layerfeat',\n",
       " 'classical.00010.wavlmbase4layerfeat',\n",
       " 'classical.00011.wavlmbase4layerfeat',\n",
       " 'classical.00012.wavlmbase4layerfeat',\n",
       " 'classical.00013.wavlmbase4layerfeat',\n",
       " 'classical.00014.wavlmbase4layerfeat',\n",
       " 'classical.00015.wavlmbase4layerfeat',\n",
       " 'classical.00016.wavlmbase4layerfeat',\n",
       " 'classical.00017.wavlmbase4layerfeat',\n",
       " 'classical.00018.wavlmbase4layerfeat',\n",
       " 'classical.00019.wavlmbase4layerfeat',\n",
       " 'classical.00020.wavlmbase4layerfeat',\n",
       " 'classical.00021.wavlmbase4layerfeat',\n",
       " 'classical.00022.wavlmbase4layerfeat',\n",
       " 'classical.00023.wavlmbase4layerfeat',\n",
       " 'classical.00024.wavlmbase4layerfeat',\n",
       " 'classical.00025.wavlmbase4layerfeat',\n",
       " 'classical.00026.wavlmbase4layerfeat',\n",
       " 'classical.00027.wavlmbase4layerfeat',\n",
       " 'classical.00028.wavlmbase4layerfeat',\n",
       " 'classical.00029.wavlmbase4layerfeat',\n",
       " 'classical.00030.wavlmbase4layerfeat',\n",
       " 'classical.00031.wavlmbase4layerfeat',\n",
       " 'classical.00032.wavlmbase4layerfeat',\n",
       " 'classical.00033.wavlmbase4layerfeat',\n",
       " 'classical.00034.wavlmbase4layerfeat',\n",
       " 'classical.00035.wavlmbase4layerfeat',\n",
       " 'classical.00036.wavlmbase4layerfeat',\n",
       " 'classical.00037.wavlmbase4layerfeat',\n",
       " 'classical.00038.wavlmbase4layerfeat',\n",
       " 'classical.00039.wavlmbase4layerfeat',\n",
       " 'classical.00040.wavlmbase4layerfeat',\n",
       " 'classical.00041.wavlmbase4layerfeat',\n",
       " 'classical.00042.wavlmbase4layerfeat',\n",
       " 'classical.00043.wavlmbase4layerfeat',\n",
       " 'classical.00044.wavlmbase4layerfeat',\n",
       " 'classical.00045.wavlmbase4layerfeat',\n",
       " 'classical.00046.wavlmbase4layerfeat',\n",
       " 'classical.00047.wavlmbase4layerfeat',\n",
       " 'classical.00048.wavlmbase4layerfeat',\n",
       " 'classical.00049.wavlmbase4layerfeat',\n",
       " 'classical.00050.wavlmbase4layerfeat',\n",
       " 'classical.00051.wavlmbase4layerfeat',\n",
       " 'classical.00052.wavlmbase4layerfeat',\n",
       " 'classical.00053.wavlmbase4layerfeat',\n",
       " 'classical.00054.wavlmbase4layerfeat',\n",
       " 'classical.00055.wavlmbase4layerfeat',\n",
       " 'classical.00056.wavlmbase4layerfeat',\n",
       " 'classical.00057.wavlmbase4layerfeat',\n",
       " 'classical.00058.wavlmbase4layerfeat',\n",
       " 'classical.00059.wavlmbase4layerfeat',\n",
       " 'classical.00060.wavlmbase4layerfeat',\n",
       " 'classical.00061.wavlmbase4layerfeat',\n",
       " 'classical.00062.wavlmbase4layerfeat',\n",
       " 'classical.00063.wavlmbase4layerfeat',\n",
       " 'classical.00064.wavlmbase4layerfeat',\n",
       " 'classical.00065.wavlmbase4layerfeat',\n",
       " 'classical.00066.wavlmbase4layerfeat',\n",
       " 'classical.00067.wavlmbase4layerfeat',\n",
       " 'classical.00068.wavlmbase4layerfeat',\n",
       " 'classical.00069.wavlmbase4layerfeat',\n",
       " 'classical.00070.wavlmbase4layerfeat',\n",
       " 'classical.00071.wavlmbase4layerfeat',\n",
       " 'classical.00072.wavlmbase4layerfeat',\n",
       " 'classical.00073.wavlmbase4layerfeat',\n",
       " 'classical.00074.wavlmbase4layerfeat',\n",
       " 'classical.00075.wavlmbase4layerfeat',\n",
       " 'classical.00076.wavlmbase4layerfeat',\n",
       " 'classical.00077.wavlmbase4layerfeat',\n",
       " 'classical.00078.wavlmbase4layerfeat',\n",
       " 'classical.00079.wavlmbase4layerfeat',\n",
       " 'classical.00080.wavlmbase4layerfeat',\n",
       " 'classical.00081.wavlmbase4layerfeat',\n",
       " 'classical.00082.wavlmbase4layerfeat',\n",
       " 'classical.00083.wavlmbase4layerfeat',\n",
       " 'classical.00084.wavlmbase4layerfeat',\n",
       " 'classical.00085.wavlmbase4layerfeat',\n",
       " 'classical.00086.wavlmbase4layerfeat',\n",
       " 'classical.00087.wavlmbase4layerfeat',\n",
       " 'classical.00088.wavlmbase4layerfeat',\n",
       " 'classical.00089.wavlmbase4layerfeat',\n",
       " 'classical.00090.wavlmbase4layerfeat',\n",
       " 'classical.00091.wavlmbase4layerfeat',\n",
       " 'classical.00092.wavlmbase4layerfeat',\n",
       " 'classical.00093.wavlmbase4layerfeat',\n",
       " 'classical.00094.wavlmbase4layerfeat',\n",
       " 'classical.00095.wavlmbase4layerfeat',\n",
       " 'classical.00096.wavlmbase4layerfeat',\n",
       " 'classical.00097.wavlmbase4layerfeat',\n",
       " 'classical.00098.wavlmbase4layerfeat',\n",
       " 'classical.00099.wavlmbase4layerfeat',\n",
       " 'country.00000.wavlmbase4layerfeat',\n",
       " 'country.00001.wavlmbase4layerfeat',\n",
       " 'country.00002.wavlmbase4layerfeat',\n",
       " 'country.00003.wavlmbase4layerfeat',\n",
       " 'country.00004.wavlmbase4layerfeat',\n",
       " 'country.00005.wavlmbase4layerfeat',\n",
       " 'country.00006.wavlmbase4layerfeat',\n",
       " 'country.00007.wavlmbase4layerfeat',\n",
       " 'country.00008.wavlmbase4layerfeat',\n",
       " 'country.00009.wavlmbase4layerfeat',\n",
       " 'country.00010.wavlmbase4layerfeat',\n",
       " 'country.00011.wavlmbase4layerfeat',\n",
       " 'country.00012.wavlmbase4layerfeat',\n",
       " 'country.00013.wavlmbase4layerfeat',\n",
       " 'country.00014.wavlmbase4layerfeat',\n",
       " 'country.00015.wavlmbase4layerfeat',\n",
       " 'country.00016.wavlmbase4layerfeat',\n",
       " 'country.00017.wavlmbase4layerfeat',\n",
       " 'country.00018.wavlmbase4layerfeat',\n",
       " 'country.00019.wavlmbase4layerfeat',\n",
       " 'country.00020.wavlmbase4layerfeat',\n",
       " 'country.00021.wavlmbase4layerfeat',\n",
       " 'country.00022.wavlmbase4layerfeat',\n",
       " 'country.00023.wavlmbase4layerfeat',\n",
       " 'country.00024.wavlmbase4layerfeat',\n",
       " 'country.00025.wavlmbase4layerfeat',\n",
       " 'country.00026.wavlmbase4layerfeat',\n",
       " 'country.00027.wavlmbase4layerfeat',\n",
       " 'country.00028.wavlmbase4layerfeat',\n",
       " 'country.00029.wavlmbase4layerfeat',\n",
       " 'country.00030.wavlmbase4layerfeat',\n",
       " 'country.00031.wavlmbase4layerfeat',\n",
       " 'country.00032.wavlmbase4layerfeat',\n",
       " 'country.00033.wavlmbase4layerfeat',\n",
       " 'country.00034.wavlmbase4layerfeat',\n",
       " 'country.00035.wavlmbase4layerfeat',\n",
       " 'country.00036.wavlmbase4layerfeat',\n",
       " 'country.00037.wavlmbase4layerfeat',\n",
       " 'country.00038.wavlmbase4layerfeat',\n",
       " 'country.00039.wavlmbase4layerfeat',\n",
       " 'country.00040.wavlmbase4layerfeat',\n",
       " 'country.00041.wavlmbase4layerfeat',\n",
       " 'country.00042.wavlmbase4layerfeat',\n",
       " 'country.00043.wavlmbase4layerfeat',\n",
       " 'country.00044.wavlmbase4layerfeat',\n",
       " 'country.00045.wavlmbase4layerfeat',\n",
       " 'country.00046.wavlmbase4layerfeat',\n",
       " 'country.00047.wavlmbase4layerfeat',\n",
       " 'country.00048.wavlmbase4layerfeat',\n",
       " 'country.00049.wavlmbase4layerfeat',\n",
       " 'country.00050.wavlmbase4layerfeat',\n",
       " 'country.00051.wavlmbase4layerfeat',\n",
       " 'country.00052.wavlmbase4layerfeat',\n",
       " 'country.00053.wavlmbase4layerfeat',\n",
       " 'country.00054.wavlmbase4layerfeat',\n",
       " 'country.00055.wavlmbase4layerfeat',\n",
       " 'country.00056.wavlmbase4layerfeat',\n",
       " 'country.00057.wavlmbase4layerfeat',\n",
       " 'country.00058.wavlmbase4layerfeat',\n",
       " 'country.00059.wavlmbase4layerfeat',\n",
       " 'country.00060.wavlmbase4layerfeat',\n",
       " 'country.00061.wavlmbase4layerfeat',\n",
       " 'country.00062.wavlmbase4layerfeat',\n",
       " 'country.00063.wavlmbase4layerfeat',\n",
       " 'country.00064.wavlmbase4layerfeat',\n",
       " 'country.00065.wavlmbase4layerfeat',\n",
       " 'country.00066.wavlmbase4layerfeat',\n",
       " 'country.00067.wavlmbase4layerfeat',\n",
       " 'country.00068.wavlmbase4layerfeat',\n",
       " 'country.00069.wavlmbase4layerfeat',\n",
       " 'country.00070.wavlmbase4layerfeat',\n",
       " 'country.00071.wavlmbase4layerfeat',\n",
       " 'country.00072.wavlmbase4layerfeat',\n",
       " 'country.00073.wavlmbase4layerfeat',\n",
       " 'country.00074.wavlmbase4layerfeat',\n",
       " 'country.00075.wavlmbase4layerfeat',\n",
       " 'country.00076.wavlmbase4layerfeat',\n",
       " 'country.00077.wavlmbase4layerfeat',\n",
       " 'country.00078.wavlmbase4layerfeat',\n",
       " 'country.00079.wavlmbase4layerfeat',\n",
       " 'country.00080.wavlmbase4layerfeat',\n",
       " 'country.00081.wavlmbase4layerfeat',\n",
       " 'country.00082.wavlmbase4layerfeat',\n",
       " 'country.00083.wavlmbase4layerfeat',\n",
       " 'country.00084.wavlmbase4layerfeat',\n",
       " 'country.00085.wavlmbase4layerfeat',\n",
       " 'country.00086.wavlmbase4layerfeat',\n",
       " 'country.00087.wavlmbase4layerfeat',\n",
       " 'country.00088.wavlmbase4layerfeat',\n",
       " 'country.00089.wavlmbase4layerfeat',\n",
       " 'country.00090.wavlmbase4layerfeat',\n",
       " 'country.00091.wavlmbase4layerfeat',\n",
       " 'country.00092.wavlmbase4layerfeat',\n",
       " 'country.00093.wavlmbase4layerfeat',\n",
       " 'country.00094.wavlmbase4layerfeat',\n",
       " 'country.00095.wavlmbase4layerfeat',\n",
       " 'country.00096.wavlmbase4layerfeat',\n",
       " 'country.00097.wavlmbase4layerfeat',\n",
       " 'country.00098.wavlmbase4layerfeat',\n",
       " 'country.00099.wavlmbase4layerfeat',\n",
       " 'disco.00000.wavlmbase4layerfeat',\n",
       " 'disco.00001.wavlmbase4layerfeat',\n",
       " 'disco.00002.wavlmbase4layerfeat',\n",
       " 'disco.00003.wavlmbase4layerfeat',\n",
       " 'disco.00004.wavlmbase4layerfeat',\n",
       " 'disco.00005.wavlmbase4layerfeat',\n",
       " 'disco.00006.wavlmbase4layerfeat',\n",
       " 'disco.00007.wavlmbase4layerfeat',\n",
       " 'disco.00008.wavlmbase4layerfeat',\n",
       " 'disco.00009.wavlmbase4layerfeat',\n",
       " 'disco.00010.wavlmbase4layerfeat',\n",
       " 'disco.00011.wavlmbase4layerfeat',\n",
       " 'disco.00012.wavlmbase4layerfeat',\n",
       " 'disco.00013.wavlmbase4layerfeat',\n",
       " 'disco.00014.wavlmbase4layerfeat',\n",
       " 'disco.00015.wavlmbase4layerfeat',\n",
       " 'disco.00016.wavlmbase4layerfeat',\n",
       " 'disco.00017.wavlmbase4layerfeat',\n",
       " 'disco.00018.wavlmbase4layerfeat',\n",
       " 'disco.00019.wavlmbase4layerfeat',\n",
       " 'disco.00020.wavlmbase4layerfeat',\n",
       " 'disco.00021.wavlmbase4layerfeat',\n",
       " 'disco.00022.wavlmbase4layerfeat',\n",
       " 'disco.00023.wavlmbase4layerfeat',\n",
       " 'disco.00024.wavlmbase4layerfeat',\n",
       " 'disco.00025.wavlmbase4layerfeat',\n",
       " 'disco.00026.wavlmbase4layerfeat',\n",
       " 'disco.00027.wavlmbase4layerfeat',\n",
       " 'disco.00028.wavlmbase4layerfeat',\n",
       " 'disco.00029.wavlmbase4layerfeat',\n",
       " 'disco.00030.wavlmbase4layerfeat',\n",
       " 'disco.00031.wavlmbase4layerfeat',\n",
       " 'disco.00032.wavlmbase4layerfeat',\n",
       " 'disco.00033.wavlmbase4layerfeat',\n",
       " 'disco.00034.wavlmbase4layerfeat',\n",
       " 'disco.00035.wavlmbase4layerfeat',\n",
       " 'disco.00036.wavlmbase4layerfeat',\n",
       " 'disco.00037.wavlmbase4layerfeat',\n",
       " 'disco.00038.wavlmbase4layerfeat',\n",
       " 'disco.00039.wavlmbase4layerfeat',\n",
       " 'disco.00040.wavlmbase4layerfeat',\n",
       " 'disco.00041.wavlmbase4layerfeat',\n",
       " 'disco.00042.wavlmbase4layerfeat',\n",
       " 'disco.00043.wavlmbase4layerfeat',\n",
       " 'disco.00044.wavlmbase4layerfeat',\n",
       " 'disco.00045.wavlmbase4layerfeat',\n",
       " 'disco.00046.wavlmbase4layerfeat',\n",
       " 'disco.00047.wavlmbase4layerfeat',\n",
       " 'disco.00048.wavlmbase4layerfeat',\n",
       " 'disco.00049.wavlmbase4layerfeat',\n",
       " 'disco.00050.wavlmbase4layerfeat',\n",
       " 'disco.00051.wavlmbase4layerfeat',\n",
       " 'disco.00052.wavlmbase4layerfeat',\n",
       " 'disco.00053.wavlmbase4layerfeat',\n",
       " 'disco.00054.wavlmbase4layerfeat',\n",
       " 'disco.00055.wavlmbase4layerfeat',\n",
       " 'disco.00056.wavlmbase4layerfeat',\n",
       " 'disco.00057.wavlmbase4layerfeat',\n",
       " 'disco.00058.wavlmbase4layerfeat',\n",
       " 'disco.00059.wavlmbase4layerfeat',\n",
       " 'disco.00060.wavlmbase4layerfeat',\n",
       " 'disco.00061.wavlmbase4layerfeat',\n",
       " 'disco.00062.wavlmbase4layerfeat',\n",
       " 'disco.00063.wavlmbase4layerfeat',\n",
       " 'disco.00064.wavlmbase4layerfeat',\n",
       " 'disco.00065.wavlmbase4layerfeat',\n",
       " 'disco.00066.wavlmbase4layerfeat',\n",
       " 'disco.00067.wavlmbase4layerfeat',\n",
       " 'disco.00068.wavlmbase4layerfeat',\n",
       " 'disco.00069.wavlmbase4layerfeat',\n",
       " 'disco.00070.wavlmbase4layerfeat',\n",
       " 'disco.00071.wavlmbase4layerfeat',\n",
       " 'disco.00072.wavlmbase4layerfeat',\n",
       " 'disco.00073.wavlmbase4layerfeat',\n",
       " 'disco.00074.wavlmbase4layerfeat',\n",
       " 'disco.00075.wavlmbase4layerfeat',\n",
       " 'disco.00076.wavlmbase4layerfeat',\n",
       " 'disco.00077.wavlmbase4layerfeat',\n",
       " 'disco.00078.wavlmbase4layerfeat',\n",
       " 'disco.00079.wavlmbase4layerfeat',\n",
       " 'disco.00080.wavlmbase4layerfeat',\n",
       " 'disco.00081.wavlmbase4layerfeat',\n",
       " 'disco.00082.wavlmbase4layerfeat',\n",
       " 'disco.00083.wavlmbase4layerfeat',\n",
       " 'disco.00084.wavlmbase4layerfeat',\n",
       " 'disco.00085.wavlmbase4layerfeat',\n",
       " 'disco.00086.wavlmbase4layerfeat',\n",
       " 'disco.00087.wavlmbase4layerfeat',\n",
       " 'disco.00088.wavlmbase4layerfeat',\n",
       " 'disco.00089.wavlmbase4layerfeat',\n",
       " 'disco.00090.wavlmbase4layerfeat',\n",
       " 'disco.00091.wavlmbase4layerfeat',\n",
       " 'disco.00092.wavlmbase4layerfeat',\n",
       " 'disco.00093.wavlmbase4layerfeat',\n",
       " 'disco.00094.wavlmbase4layerfeat',\n",
       " 'disco.00095.wavlmbase4layerfeat',\n",
       " 'disco.00096.wavlmbase4layerfeat',\n",
       " 'disco.00097.wavlmbase4layerfeat',\n",
       " 'disco.00098.wavlmbase4layerfeat',\n",
       " 'disco.00099.wavlmbase4layerfeat',\n",
       " 'hiphop.00000.wavlmbase4layerfeat',\n",
       " 'hiphop.00001.wavlmbase4layerfeat',\n",
       " 'hiphop.00002.wavlmbase4layerfeat',\n",
       " 'hiphop.00003.wavlmbase4layerfeat',\n",
       " 'hiphop.00004.wavlmbase4layerfeat',\n",
       " 'hiphop.00005.wavlmbase4layerfeat',\n",
       " 'hiphop.00006.wavlmbase4layerfeat',\n",
       " 'hiphop.00007.wavlmbase4layerfeat',\n",
       " 'hiphop.00008.wavlmbase4layerfeat',\n",
       " 'hiphop.00009.wavlmbase4layerfeat',\n",
       " 'hiphop.00010.wavlmbase4layerfeat',\n",
       " 'hiphop.00011.wavlmbase4layerfeat',\n",
       " 'hiphop.00012.wavlmbase4layerfeat',\n",
       " 'hiphop.00013.wavlmbase4layerfeat',\n",
       " 'hiphop.00014.wavlmbase4layerfeat',\n",
       " 'hiphop.00015.wavlmbase4layerfeat',\n",
       " 'hiphop.00016.wavlmbase4layerfeat',\n",
       " 'hiphop.00017.wavlmbase4layerfeat',\n",
       " 'hiphop.00018.wavlmbase4layerfeat',\n",
       " 'hiphop.00019.wavlmbase4layerfeat',\n",
       " 'hiphop.00020.wavlmbase4layerfeat',\n",
       " 'hiphop.00021.wavlmbase4layerfeat',\n",
       " 'hiphop.00022.wavlmbase4layerfeat',\n",
       " 'hiphop.00023.wavlmbase4layerfeat',\n",
       " 'hiphop.00024.wavlmbase4layerfeat',\n",
       " 'hiphop.00025.wavlmbase4layerfeat',\n",
       " 'hiphop.00026.wavlmbase4layerfeat',\n",
       " 'hiphop.00027.wavlmbase4layerfeat',\n",
       " 'hiphop.00028.wavlmbase4layerfeat',\n",
       " 'hiphop.00029.wavlmbase4layerfeat',\n",
       " 'hiphop.00030.wavlmbase4layerfeat',\n",
       " 'hiphop.00031.wavlmbase4layerfeat',\n",
       " 'hiphop.00032.wavlmbase4layerfeat',\n",
       " 'hiphop.00033.wavlmbase4layerfeat',\n",
       " 'hiphop.00034.wavlmbase4layerfeat',\n",
       " 'hiphop.00035.wavlmbase4layerfeat',\n",
       " 'hiphop.00036.wavlmbase4layerfeat',\n",
       " 'hiphop.00037.wavlmbase4layerfeat',\n",
       " 'hiphop.00038.wavlmbase4layerfeat',\n",
       " 'hiphop.00039.wavlmbase4layerfeat',\n",
       " 'hiphop.00040.wavlmbase4layerfeat',\n",
       " 'hiphop.00041.wavlmbase4layerfeat',\n",
       " 'hiphop.00042.wavlmbase4layerfeat',\n",
       " 'hiphop.00043.wavlmbase4layerfeat',\n",
       " 'hiphop.00044.wavlmbase4layerfeat',\n",
       " 'hiphop.00045.wavlmbase4layerfeat',\n",
       " 'hiphop.00046.wavlmbase4layerfeat',\n",
       " 'hiphop.00047.wavlmbase4layerfeat',\n",
       " 'hiphop.00048.wavlmbase4layerfeat',\n",
       " 'hiphop.00049.wavlmbase4layerfeat',\n",
       " 'hiphop.00050.wavlmbase4layerfeat',\n",
       " 'hiphop.00051.wavlmbase4layerfeat',\n",
       " 'hiphop.00052.wavlmbase4layerfeat',\n",
       " 'hiphop.00053.wavlmbase4layerfeat',\n",
       " 'hiphop.00054.wavlmbase4layerfeat',\n",
       " 'hiphop.00055.wavlmbase4layerfeat',\n",
       " 'hiphop.00056.wavlmbase4layerfeat',\n",
       " 'hiphop.00057.wavlmbase4layerfeat',\n",
       " 'hiphop.00058.wavlmbase4layerfeat',\n",
       " 'hiphop.00059.wavlmbase4layerfeat',\n",
       " 'hiphop.00060.wavlmbase4layerfeat',\n",
       " 'hiphop.00061.wavlmbase4layerfeat',\n",
       " 'hiphop.00062.wavlmbase4layerfeat',\n",
       " 'hiphop.00063.wavlmbase4layerfeat',\n",
       " 'hiphop.00064.wavlmbase4layerfeat',\n",
       " 'hiphop.00065.wavlmbase4layerfeat',\n",
       " 'hiphop.00066.wavlmbase4layerfeat',\n",
       " 'hiphop.00067.wavlmbase4layerfeat',\n",
       " 'hiphop.00068.wavlmbase4layerfeat',\n",
       " 'hiphop.00069.wavlmbase4layerfeat',\n",
       " 'hiphop.00070.wavlmbase4layerfeat',\n",
       " 'hiphop.00071.wavlmbase4layerfeat',\n",
       " 'hiphop.00072.wavlmbase4layerfeat',\n",
       " 'hiphop.00073.wavlmbase4layerfeat',\n",
       " 'hiphop.00074.wavlmbase4layerfeat',\n",
       " 'hiphop.00075.wavlmbase4layerfeat',\n",
       " 'hiphop.00076.wavlmbase4layerfeat',\n",
       " 'hiphop.00077.wavlmbase4layerfeat',\n",
       " 'hiphop.00078.wavlmbase4layerfeat',\n",
       " 'hiphop.00079.wavlmbase4layerfeat',\n",
       " 'hiphop.00080.wavlmbase4layerfeat',\n",
       " 'hiphop.00081.wavlmbase4layerfeat',\n",
       " 'hiphop.00082.wavlmbase4layerfeat',\n",
       " 'hiphop.00083.wavlmbase4layerfeat',\n",
       " 'hiphop.00084.wavlmbase4layerfeat',\n",
       " 'hiphop.00085.wavlmbase4layerfeat',\n",
       " 'hiphop.00086.wavlmbase4layerfeat',\n",
       " 'hiphop.00087.wavlmbase4layerfeat',\n",
       " 'hiphop.00088.wavlmbase4layerfeat',\n",
       " 'hiphop.00089.wavlmbase4layerfeat',\n",
       " 'hiphop.00090.wavlmbase4layerfeat',\n",
       " 'hiphop.00091.wavlmbase4layerfeat',\n",
       " 'hiphop.00092.wavlmbase4layerfeat',\n",
       " 'hiphop.00093.wavlmbase4layerfeat',\n",
       " 'hiphop.00094.wavlmbase4layerfeat',\n",
       " 'hiphop.00095.wavlmbase4layerfeat',\n",
       " 'hiphop.00096.wavlmbase4layerfeat',\n",
       " 'hiphop.00097.wavlmbase4layerfeat',\n",
       " 'hiphop.00098.wavlmbase4layerfeat',\n",
       " 'hiphop.00099.wavlmbase4layerfeat',\n",
       " 'jazz.00000.wavlmbase4layerfeat',\n",
       " 'jazz.00001.wavlmbase4layerfeat',\n",
       " 'jazz.00002.wavlmbase4layerfeat',\n",
       " 'jazz.00003.wavlmbase4layerfeat',\n",
       " 'jazz.00004.wavlmbase4layerfeat',\n",
       " 'jazz.00005.wavlmbase4layerfeat',\n",
       " 'jazz.00006.wavlmbase4layerfeat',\n",
       " 'jazz.00007.wavlmbase4layerfeat',\n",
       " 'jazz.00008.wavlmbase4layerfeat',\n",
       " 'jazz.00009.wavlmbase4layerfeat',\n",
       " 'jazz.00010.wavlmbase4layerfeat',\n",
       " 'jazz.00011.wavlmbase4layerfeat',\n",
       " 'jazz.00012.wavlmbase4layerfeat',\n",
       " 'jazz.00013.wavlmbase4layerfeat',\n",
       " 'jazz.00014.wavlmbase4layerfeat',\n",
       " 'jazz.00015.wavlmbase4layerfeat',\n",
       " 'jazz.00016.wavlmbase4layerfeat',\n",
       " 'jazz.00017.wavlmbase4layerfeat',\n",
       " 'jazz.00018.wavlmbase4layerfeat',\n",
       " 'jazz.00019.wavlmbase4layerfeat',\n",
       " 'jazz.00020.wavlmbase4layerfeat',\n",
       " 'jazz.00021.wavlmbase4layerfeat',\n",
       " 'jazz.00022.wavlmbase4layerfeat',\n",
       " 'jazz.00023.wavlmbase4layerfeat',\n",
       " 'jazz.00024.wavlmbase4layerfeat',\n",
       " 'jazz.00025.wavlmbase4layerfeat',\n",
       " 'jazz.00026.wavlmbase4layerfeat',\n",
       " 'jazz.00027.wavlmbase4layerfeat',\n",
       " 'jazz.00028.wavlmbase4layerfeat',\n",
       " 'jazz.00029.wavlmbase4layerfeat',\n",
       " 'jazz.00030.wavlmbase4layerfeat',\n",
       " 'jazz.00031.wavlmbase4layerfeat',\n",
       " 'jazz.00032.wavlmbase4layerfeat',\n",
       " 'jazz.00033.wavlmbase4layerfeat',\n",
       " 'jazz.00034.wavlmbase4layerfeat',\n",
       " 'jazz.00035.wavlmbase4layerfeat',\n",
       " 'jazz.00036.wavlmbase4layerfeat',\n",
       " 'jazz.00037.wavlmbase4layerfeat',\n",
       " 'jazz.00038.wavlmbase4layerfeat',\n",
       " 'jazz.00039.wavlmbase4layerfeat',\n",
       " 'jazz.00040.wavlmbase4layerfeat',\n",
       " 'jazz.00041.wavlmbase4layerfeat',\n",
       " 'jazz.00042.wavlmbase4layerfeat',\n",
       " 'jazz.00043.wavlmbase4layerfeat',\n",
       " 'jazz.00044.wavlmbase4layerfeat',\n",
       " 'jazz.00045.wavlmbase4layerfeat',\n",
       " 'jazz.00046.wavlmbase4layerfeat',\n",
       " 'jazz.00047.wavlmbase4layerfeat',\n",
       " 'jazz.00048.wavlmbase4layerfeat',\n",
       " 'jazz.00049.wavlmbase4layerfeat',\n",
       " 'jazz.00050.wavlmbase4layerfeat',\n",
       " 'jazz.00051.wavlmbase4layerfeat',\n",
       " 'jazz.00052.wavlmbase4layerfeat',\n",
       " 'jazz.00053.wavlmbase4layerfeat',\n",
       " 'jazz.00054.wavlmbase4layerfeat',\n",
       " 'jazz.00055.wavlmbase4layerfeat',\n",
       " 'jazz.00056.wavlmbase4layerfeat',\n",
       " 'jazz.00057.wavlmbase4layerfeat',\n",
       " 'jazz.00058.wavlmbase4layerfeat',\n",
       " 'jazz.00059.wavlmbase4layerfeat',\n",
       " 'jazz.00060.wavlmbase4layerfeat',\n",
       " 'jazz.00061.wavlmbase4layerfeat',\n",
       " 'jazz.00062.wavlmbase4layerfeat',\n",
       " 'jazz.00063.wavlmbase4layerfeat',\n",
       " 'jazz.00064.wavlmbase4layerfeat',\n",
       " 'jazz.00065.wavlmbase4layerfeat',\n",
       " 'jazz.00066.wavlmbase4layerfeat',\n",
       " 'jazz.00067.wavlmbase4layerfeat',\n",
       " 'jazz.00068.wavlmbase4layerfeat',\n",
       " 'jazz.00069.wavlmbase4layerfeat',\n",
       " 'jazz.00070.wavlmbase4layerfeat',\n",
       " 'jazz.00071.wavlmbase4layerfeat',\n",
       " 'jazz.00072.wavlmbase4layerfeat',\n",
       " 'jazz.00073.wavlmbase4layerfeat',\n",
       " 'jazz.00074.wavlmbase4layerfeat',\n",
       " 'jazz.00075.wavlmbase4layerfeat',\n",
       " 'jazz.00076.wavlmbase4layerfeat',\n",
       " 'jazz.00077.wavlmbase4layerfeat',\n",
       " 'jazz.00078.wavlmbase4layerfeat',\n",
       " 'jazz.00079.wavlmbase4layerfeat',\n",
       " 'jazz.00080.wavlmbase4layerfeat',\n",
       " 'jazz.00081.wavlmbase4layerfeat',\n",
       " 'jazz.00082.wavlmbase4layerfeat',\n",
       " 'jazz.00083.wavlmbase4layerfeat',\n",
       " 'jazz.00084.wavlmbase4layerfeat',\n",
       " 'jazz.00085.wavlmbase4layerfeat',\n",
       " 'jazz.00086.wavlmbase4layerfeat',\n",
       " 'jazz.00087.wavlmbase4layerfeat',\n",
       " 'jazz.00088.wavlmbase4layerfeat',\n",
       " 'jazz.00089.wavlmbase4layerfeat',\n",
       " 'jazz.00090.wavlmbase4layerfeat',\n",
       " 'jazz.00091.wavlmbase4layerfeat',\n",
       " 'jazz.00092.wavlmbase4layerfeat',\n",
       " 'jazz.00093.wavlmbase4layerfeat',\n",
       " 'jazz.00094.wavlmbase4layerfeat',\n",
       " 'jazz.00095.wavlmbase4layerfeat',\n",
       " 'jazz.00096.wavlmbase4layerfeat',\n",
       " 'jazz.00097.wavlmbase4layerfeat',\n",
       " 'jazz.00098.wavlmbase4layerfeat',\n",
       " 'jazz.00099.wavlmbase4layerfeat',\n",
       " 'metal.00000.wavlmbase4layerfeat',\n",
       " 'metal.00001.wavlmbase4layerfeat',\n",
       " 'metal.00002.wavlmbase4layerfeat',\n",
       " 'metal.00003.wavlmbase4layerfeat',\n",
       " 'metal.00004.wavlmbase4layerfeat',\n",
       " 'metal.00005.wavlmbase4layerfeat',\n",
       " 'metal.00006.wavlmbase4layerfeat',\n",
       " 'metal.00007.wavlmbase4layerfeat',\n",
       " 'metal.00008.wavlmbase4layerfeat',\n",
       " 'metal.00009.wavlmbase4layerfeat',\n",
       " 'metal.00010.wavlmbase4layerfeat',\n",
       " 'metal.00011.wavlmbase4layerfeat',\n",
       " 'metal.00012.wavlmbase4layerfeat',\n",
       " 'metal.00013.wavlmbase4layerfeat',\n",
       " 'metal.00014.wavlmbase4layerfeat',\n",
       " 'metal.00015.wavlmbase4layerfeat',\n",
       " 'metal.00016.wavlmbase4layerfeat',\n",
       " 'metal.00017.wavlmbase4layerfeat',\n",
       " 'metal.00018.wavlmbase4layerfeat',\n",
       " 'metal.00019.wavlmbase4layerfeat',\n",
       " 'metal.00020.wavlmbase4layerfeat',\n",
       " 'metal.00021.wavlmbase4layerfeat',\n",
       " 'metal.00022.wavlmbase4layerfeat',\n",
       " 'metal.00023.wavlmbase4layerfeat',\n",
       " 'metal.00024.wavlmbase4layerfeat',\n",
       " 'metal.00025.wavlmbase4layerfeat',\n",
       " 'metal.00026.wavlmbase4layerfeat',\n",
       " 'metal.00027.wavlmbase4layerfeat',\n",
       " 'metal.00028.wavlmbase4layerfeat',\n",
       " 'metal.00029.wavlmbase4layerfeat',\n",
       " 'metal.00030.wavlmbase4layerfeat',\n",
       " 'metal.00031.wavlmbase4layerfeat',\n",
       " 'metal.00032.wavlmbase4layerfeat',\n",
       " 'metal.00033.wavlmbase4layerfeat',\n",
       " 'metal.00034.wavlmbase4layerfeat',\n",
       " 'metal.00035.wavlmbase4layerfeat',\n",
       " 'metal.00036.wavlmbase4layerfeat',\n",
       " 'metal.00037.wavlmbase4layerfeat',\n",
       " 'metal.00038.wavlmbase4layerfeat',\n",
       " 'metal.00039.wavlmbase4layerfeat',\n",
       " 'metal.00040.wavlmbase4layerfeat',\n",
       " 'metal.00041.wavlmbase4layerfeat',\n",
       " 'metal.00042.wavlmbase4layerfeat',\n",
       " 'metal.00043.wavlmbase4layerfeat',\n",
       " 'metal.00044.wavlmbase4layerfeat',\n",
       " 'metal.00045.wavlmbase4layerfeat',\n",
       " 'metal.00046.wavlmbase4layerfeat',\n",
       " 'metal.00047.wavlmbase4layerfeat',\n",
       " 'metal.00048.wavlmbase4layerfeat',\n",
       " 'metal.00049.wavlmbase4layerfeat',\n",
       " 'metal.00050.wavlmbase4layerfeat',\n",
       " 'metal.00051.wavlmbase4layerfeat',\n",
       " 'metal.00052.wavlmbase4layerfeat',\n",
       " 'metal.00053.wavlmbase4layerfeat',\n",
       " 'metal.00054.wavlmbase4layerfeat',\n",
       " 'metal.00055.wavlmbase4layerfeat',\n",
       " 'metal.00056.wavlmbase4layerfeat',\n",
       " 'metal.00057.wavlmbase4layerfeat',\n",
       " 'metal.00058.wavlmbase4layerfeat',\n",
       " 'metal.00059.wavlmbase4layerfeat',\n",
       " 'metal.00060.wavlmbase4layerfeat',\n",
       " 'metal.00061.wavlmbase4layerfeat',\n",
       " 'metal.00062.wavlmbase4layerfeat',\n",
       " 'metal.00063.wavlmbase4layerfeat',\n",
       " 'metal.00064.wavlmbase4layerfeat',\n",
       " 'metal.00065.wavlmbase4layerfeat',\n",
       " 'metal.00066.wavlmbase4layerfeat',\n",
       " 'metal.00067.wavlmbase4layerfeat',\n",
       " 'metal.00068.wavlmbase4layerfeat',\n",
       " 'metal.00069.wavlmbase4layerfeat',\n",
       " 'metal.00070.wavlmbase4layerfeat',\n",
       " 'metal.00071.wavlmbase4layerfeat',\n",
       " 'metal.00072.wavlmbase4layerfeat',\n",
       " 'metal.00073.wavlmbase4layerfeat',\n",
       " 'metal.00074.wavlmbase4layerfeat',\n",
       " 'metal.00075.wavlmbase4layerfeat',\n",
       " 'metal.00076.wavlmbase4layerfeat',\n",
       " 'metal.00077.wavlmbase4layerfeat',\n",
       " 'metal.00078.wavlmbase4layerfeat',\n",
       " 'metal.00079.wavlmbase4layerfeat',\n",
       " 'metal.00080.wavlmbase4layerfeat',\n",
       " 'metal.00081.wavlmbase4layerfeat',\n",
       " 'metal.00082.wavlmbase4layerfeat',\n",
       " 'metal.00083.wavlmbase4layerfeat',\n",
       " 'metal.00084.wavlmbase4layerfeat',\n",
       " 'metal.00085.wavlmbase4layerfeat',\n",
       " 'metal.00086.wavlmbase4layerfeat',\n",
       " 'metal.00087.wavlmbase4layerfeat',\n",
       " 'metal.00088.wavlmbase4layerfeat',\n",
       " 'metal.00089.wavlmbase4layerfeat',\n",
       " 'metal.00090.wavlmbase4layerfeat',\n",
       " 'metal.00091.wavlmbase4layerfeat',\n",
       " 'metal.00092.wavlmbase4layerfeat',\n",
       " 'metal.00093.wavlmbase4layerfeat',\n",
       " 'metal.00094.wavlmbase4layerfeat',\n",
       " 'metal.00095.wavlmbase4layerfeat',\n",
       " 'metal.00096.wavlmbase4layerfeat',\n",
       " 'metal.00097.wavlmbase4layerfeat',\n",
       " 'metal.00098.wavlmbase4layerfeat',\n",
       " 'metal.00099.wavlmbase4layerfeat',\n",
       " 'pop.00000.wavlmbase4layerfeat',\n",
       " 'pop.00001.wavlmbase4layerfeat',\n",
       " 'pop.00002.wavlmbase4layerfeat',\n",
       " 'pop.00003.wavlmbase4layerfeat',\n",
       " 'pop.00004.wavlmbase4layerfeat',\n",
       " 'pop.00005.wavlmbase4layerfeat',\n",
       " 'pop.00006.wavlmbase4layerfeat',\n",
       " 'pop.00007.wavlmbase4layerfeat',\n",
       " 'pop.00008.wavlmbase4layerfeat',\n",
       " 'pop.00009.wavlmbase4layerfeat',\n",
       " 'pop.00010.wavlmbase4layerfeat',\n",
       " 'pop.00011.wavlmbase4layerfeat',\n",
       " 'pop.00012.wavlmbase4layerfeat',\n",
       " 'pop.00013.wavlmbase4layerfeat',\n",
       " 'pop.00014.wavlmbase4layerfeat',\n",
       " 'pop.00015.wavlmbase4layerfeat',\n",
       " 'pop.00016.wavlmbase4layerfeat',\n",
       " 'pop.00017.wavlmbase4layerfeat',\n",
       " 'pop.00018.wavlmbase4layerfeat',\n",
       " 'pop.00019.wavlmbase4layerfeat',\n",
       " 'pop.00020.wavlmbase4layerfeat',\n",
       " 'pop.00021.wavlmbase4layerfeat',\n",
       " 'pop.00022.wavlmbase4layerfeat',\n",
       " 'pop.00023.wavlmbase4layerfeat',\n",
       " 'pop.00024.wavlmbase4layerfeat',\n",
       " 'pop.00025.wavlmbase4layerfeat',\n",
       " 'pop.00026.wavlmbase4layerfeat',\n",
       " 'pop.00027.wavlmbase4layerfeat',\n",
       " 'pop.00028.wavlmbase4layerfeat',\n",
       " 'pop.00029.wavlmbase4layerfeat',\n",
       " 'pop.00030.wavlmbase4layerfeat',\n",
       " 'pop.00031.wavlmbase4layerfeat',\n",
       " 'pop.00032.wavlmbase4layerfeat',\n",
       " 'pop.00033.wavlmbase4layerfeat',\n",
       " 'pop.00034.wavlmbase4layerfeat',\n",
       " 'pop.00035.wavlmbase4layerfeat',\n",
       " 'pop.00036.wavlmbase4layerfeat',\n",
       " 'pop.00037.wavlmbase4layerfeat',\n",
       " 'pop.00038.wavlmbase4layerfeat',\n",
       " 'pop.00039.wavlmbase4layerfeat',\n",
       " 'pop.00040.wavlmbase4layerfeat',\n",
       " 'pop.00041.wavlmbase4layerfeat',\n",
       " 'pop.00042.wavlmbase4layerfeat',\n",
       " 'pop.00043.wavlmbase4layerfeat',\n",
       " 'pop.00044.wavlmbase4layerfeat',\n",
       " 'pop.00045.wavlmbase4layerfeat',\n",
       " 'pop.00046.wavlmbase4layerfeat',\n",
       " 'pop.00047.wavlmbase4layerfeat',\n",
       " 'pop.00048.wavlmbase4layerfeat',\n",
       " 'pop.00049.wavlmbase4layerfeat',\n",
       " 'pop.00050.wavlmbase4layerfeat',\n",
       " 'pop.00051.wavlmbase4layerfeat',\n",
       " 'pop.00052.wavlmbase4layerfeat',\n",
       " 'pop.00053.wavlmbase4layerfeat',\n",
       " 'pop.00054.wavlmbase4layerfeat',\n",
       " 'pop.00055.wavlmbase4layerfeat',\n",
       " 'pop.00056.wavlmbase4layerfeat',\n",
       " 'pop.00057.wavlmbase4layerfeat',\n",
       " 'pop.00058.wavlmbase4layerfeat',\n",
       " 'pop.00059.wavlmbase4layerfeat',\n",
       " 'pop.00060.wavlmbase4layerfeat',\n",
       " 'pop.00061.wavlmbase4layerfeat',\n",
       " 'pop.00062.wavlmbase4layerfeat',\n",
       " 'pop.00063.wavlmbase4layerfeat',\n",
       " 'pop.00064.wavlmbase4layerfeat',\n",
       " 'pop.00065.wavlmbase4layerfeat',\n",
       " 'pop.00066.wavlmbase4layerfeat',\n",
       " 'pop.00067.wavlmbase4layerfeat',\n",
       " 'pop.00068.wavlmbase4layerfeat',\n",
       " 'pop.00069.wavlmbase4layerfeat',\n",
       " 'pop.00070.wavlmbase4layerfeat',\n",
       " 'pop.00071.wavlmbase4layerfeat',\n",
       " 'pop.00072.wavlmbase4layerfeat',\n",
       " 'pop.00073.wavlmbase4layerfeat',\n",
       " 'pop.00074.wavlmbase4layerfeat',\n",
       " 'pop.00075.wavlmbase4layerfeat',\n",
       " 'pop.00076.wavlmbase4layerfeat',\n",
       " 'pop.00077.wavlmbase4layerfeat',\n",
       " 'pop.00078.wavlmbase4layerfeat',\n",
       " 'pop.00079.wavlmbase4layerfeat',\n",
       " 'pop.00080.wavlmbase4layerfeat',\n",
       " 'pop.00081.wavlmbase4layerfeat',\n",
       " 'pop.00082.wavlmbase4layerfeat',\n",
       " 'pop.00083.wavlmbase4layerfeat',\n",
       " 'pop.00084.wavlmbase4layerfeat',\n",
       " 'pop.00085.wavlmbase4layerfeat',\n",
       " 'pop.00086.wavlmbase4layerfeat',\n",
       " 'pop.00087.wavlmbase4layerfeat',\n",
       " 'pop.00088.wavlmbase4layerfeat',\n",
       " 'pop.00089.wavlmbase4layerfeat',\n",
       " 'pop.00090.wavlmbase4layerfeat',\n",
       " 'pop.00091.wavlmbase4layerfeat',\n",
       " 'pop.00092.wavlmbase4layerfeat',\n",
       " 'pop.00093.wavlmbase4layerfeat',\n",
       " 'pop.00094.wavlmbase4layerfeat',\n",
       " 'pop.00095.wavlmbase4layerfeat',\n",
       " 'pop.00096.wavlmbase4layerfeat',\n",
       " 'pop.00097.wavlmbase4layerfeat',\n",
       " 'pop.00098.wavlmbase4layerfeat',\n",
       " 'pop.00099.wavlmbase4layerfeat',\n",
       " 'reggae.00000.wavlmbase4layerfeat',\n",
       " 'reggae.00001.wavlmbase4layerfeat',\n",
       " 'reggae.00002.wavlmbase4layerfeat',\n",
       " 'reggae.00003.wavlmbase4layerfeat',\n",
       " 'reggae.00004.wavlmbase4layerfeat',\n",
       " 'reggae.00005.wavlmbase4layerfeat',\n",
       " 'reggae.00006.wavlmbase4layerfeat',\n",
       " 'reggae.00007.wavlmbase4layerfeat',\n",
       " 'reggae.00008.wavlmbase4layerfeat',\n",
       " 'reggae.00009.wavlmbase4layerfeat',\n",
       " 'reggae.00010.wavlmbase4layerfeat',\n",
       " 'reggae.00011.wavlmbase4layerfeat',\n",
       " 'reggae.00012.wavlmbase4layerfeat',\n",
       " 'reggae.00013.wavlmbase4layerfeat',\n",
       " 'reggae.00014.wavlmbase4layerfeat',\n",
       " 'reggae.00015.wavlmbase4layerfeat',\n",
       " 'reggae.00016.wavlmbase4layerfeat',\n",
       " 'reggae.00017.wavlmbase4layerfeat',\n",
       " 'reggae.00018.wavlmbase4layerfeat',\n",
       " 'reggae.00019.wavlmbase4layerfeat',\n",
       " 'reggae.00020.wavlmbase4layerfeat',\n",
       " 'reggae.00021.wavlmbase4layerfeat',\n",
       " 'reggae.00022.wavlmbase4layerfeat',\n",
       " 'reggae.00023.wavlmbase4layerfeat',\n",
       " 'reggae.00024.wavlmbase4layerfeat',\n",
       " 'reggae.00025.wavlmbase4layerfeat',\n",
       " 'reggae.00026.wavlmbase4layerfeat',\n",
       " 'reggae.00027.wavlmbase4layerfeat',\n",
       " 'reggae.00028.wavlmbase4layerfeat',\n",
       " 'reggae.00029.wavlmbase4layerfeat',\n",
       " 'reggae.00030.wavlmbase4layerfeat',\n",
       " 'reggae.00031.wavlmbase4layerfeat',\n",
       " 'reggae.00032.wavlmbase4layerfeat',\n",
       " 'reggae.00033.wavlmbase4layerfeat',\n",
       " 'reggae.00034.wavlmbase4layerfeat',\n",
       " 'reggae.00035.wavlmbase4layerfeat',\n",
       " 'reggae.00036.wavlmbase4layerfeat',\n",
       " 'reggae.00037.wavlmbase4layerfeat',\n",
       " 'reggae.00038.wavlmbase4layerfeat',\n",
       " 'reggae.00039.wavlmbase4layerfeat',\n",
       " 'reggae.00040.wavlmbase4layerfeat',\n",
       " 'reggae.00041.wavlmbase4layerfeat',\n",
       " 'reggae.00042.wavlmbase4layerfeat',\n",
       " 'reggae.00043.wavlmbase4layerfeat',\n",
       " 'reggae.00044.wavlmbase4layerfeat',\n",
       " 'reggae.00045.wavlmbase4layerfeat',\n",
       " 'reggae.00046.wavlmbase4layerfeat',\n",
       " 'reggae.00047.wavlmbase4layerfeat',\n",
       " 'reggae.00048.wavlmbase4layerfeat',\n",
       " 'reggae.00049.wavlmbase4layerfeat',\n",
       " 'reggae.00050.wavlmbase4layerfeat',\n",
       " 'reggae.00051.wavlmbase4layerfeat',\n",
       " 'reggae.00052.wavlmbase4layerfeat',\n",
       " 'reggae.00053.wavlmbase4layerfeat',\n",
       " 'reggae.00054.wavlmbase4layerfeat',\n",
       " 'reggae.00055.wavlmbase4layerfeat',\n",
       " 'reggae.00056.wavlmbase4layerfeat',\n",
       " 'reggae.00057.wavlmbase4layerfeat',\n",
       " 'reggae.00058.wavlmbase4layerfeat',\n",
       " 'reggae.00059.wavlmbase4layerfeat',\n",
       " 'reggae.00060.wavlmbase4layerfeat',\n",
       " 'reggae.00061.wavlmbase4layerfeat',\n",
       " 'reggae.00062.wavlmbase4layerfeat',\n",
       " 'reggae.00063.wavlmbase4layerfeat',\n",
       " 'reggae.00064.wavlmbase4layerfeat',\n",
       " 'reggae.00065.wavlmbase4layerfeat',\n",
       " 'reggae.00066.wavlmbase4layerfeat',\n",
       " 'reggae.00067.wavlmbase4layerfeat',\n",
       " 'reggae.00068.wavlmbase4layerfeat',\n",
       " 'reggae.00069.wavlmbase4layerfeat',\n",
       " 'reggae.00070.wavlmbase4layerfeat',\n",
       " 'reggae.00071.wavlmbase4layerfeat',\n",
       " 'reggae.00072.wavlmbase4layerfeat',\n",
       " 'reggae.00073.wavlmbase4layerfeat',\n",
       " 'reggae.00074.wavlmbase4layerfeat',\n",
       " 'reggae.00075.wavlmbase4layerfeat',\n",
       " 'reggae.00076.wavlmbase4layerfeat',\n",
       " 'reggae.00077.wavlmbase4layerfeat',\n",
       " 'reggae.00078.wavlmbase4layerfeat',\n",
       " 'reggae.00079.wavlmbase4layerfeat',\n",
       " 'reggae.00080.wavlmbase4layerfeat',\n",
       " 'reggae.00081.wavlmbase4layerfeat',\n",
       " 'reggae.00082.wavlmbase4layerfeat',\n",
       " 'reggae.00083.wavlmbase4layerfeat',\n",
       " 'reggae.00084.wavlmbase4layerfeat',\n",
       " 'reggae.00085.wavlmbase4layerfeat',\n",
       " 'reggae.00086.wavlmbase4layerfeat',\n",
       " 'reggae.00087.wavlmbase4layerfeat',\n",
       " 'reggae.00088.wavlmbase4layerfeat',\n",
       " 'reggae.00089.wavlmbase4layerfeat',\n",
       " 'reggae.00090.wavlmbase4layerfeat',\n",
       " 'reggae.00091.wavlmbase4layerfeat',\n",
       " 'reggae.00092.wavlmbase4layerfeat',\n",
       " 'reggae.00093.wavlmbase4layerfeat',\n",
       " 'reggae.00094.wavlmbase4layerfeat',\n",
       " 'reggae.00095.wavlmbase4layerfeat',\n",
       " 'reggae.00096.wavlmbase4layerfeat',\n",
       " 'reggae.00097.wavlmbase4layerfeat',\n",
       " 'reggae.00098.wavlmbase4layerfeat',\n",
       " 'reggae.00099.wavlmbase4layerfeat',\n",
       " 'rock.00000.wavlmbase4layerfeat',\n",
       " 'rock.00001.wavlmbase4layerfeat',\n",
       " 'rock.00002.wavlmbase4layerfeat',\n",
       " 'rock.00003.wavlmbase4layerfeat',\n",
       " 'rock.00004.wavlmbase4layerfeat',\n",
       " 'rock.00005.wavlmbase4layerfeat',\n",
       " 'rock.00006.wavlmbase4layerfeat',\n",
       " 'rock.00007.wavlmbase4layerfeat',\n",
       " 'rock.00008.wavlmbase4layerfeat',\n",
       " 'rock.00009.wavlmbase4layerfeat',\n",
       " 'rock.00010.wavlmbase4layerfeat',\n",
       " 'rock.00011.wavlmbase4layerfeat',\n",
       " 'rock.00012.wavlmbase4layerfeat',\n",
       " 'rock.00013.wavlmbase4layerfeat',\n",
       " 'rock.00014.wavlmbase4layerfeat',\n",
       " 'rock.00015.wavlmbase4layerfeat',\n",
       " 'rock.00016.wavlmbase4layerfeat',\n",
       " 'rock.00017.wavlmbase4layerfeat',\n",
       " 'rock.00018.wavlmbase4layerfeat',\n",
       " 'rock.00019.wavlmbase4layerfeat',\n",
       " 'rock.00020.wavlmbase4layerfeat',\n",
       " 'rock.00021.wavlmbase4layerfeat',\n",
       " 'rock.00022.wavlmbase4layerfeat',\n",
       " 'rock.00023.wavlmbase4layerfeat',\n",
       " 'rock.00024.wavlmbase4layerfeat',\n",
       " 'rock.00025.wavlmbase4layerfeat',\n",
       " 'rock.00026.wavlmbase4layerfeat',\n",
       " 'rock.00027.wavlmbase4layerfeat',\n",
       " 'rock.00028.wavlmbase4layerfeat',\n",
       " 'rock.00029.wavlmbase4layerfeat',\n",
       " 'rock.00030.wavlmbase4layerfeat',\n",
       " 'rock.00031.wavlmbase4layerfeat',\n",
       " 'rock.00032.wavlmbase4layerfeat',\n",
       " 'rock.00033.wavlmbase4layerfeat',\n",
       " 'rock.00034.wavlmbase4layerfeat',\n",
       " 'rock.00035.wavlmbase4layerfeat',\n",
       " 'rock.00036.wavlmbase4layerfeat',\n",
       " 'rock.00037.wavlmbase4layerfeat',\n",
       " 'rock.00038.wavlmbase4layerfeat',\n",
       " 'rock.00039.wavlmbase4layerfeat',\n",
       " 'rock.00040.wavlmbase4layerfeat',\n",
       " 'rock.00041.wavlmbase4layerfeat',\n",
       " 'rock.00042.wavlmbase4layerfeat',\n",
       " 'rock.00043.wavlmbase4layerfeat',\n",
       " 'rock.00044.wavlmbase4layerfeat',\n",
       " 'rock.00045.wavlmbase4layerfeat',\n",
       " 'rock.00046.wavlmbase4layerfeat',\n",
       " 'rock.00047.wavlmbase4layerfeat',\n",
       " 'rock.00048.wavlmbase4layerfeat',\n",
       " 'rock.00049.wavlmbase4layerfeat',\n",
       " 'rock.00050.wavlmbase4layerfeat',\n",
       " 'rock.00051.wavlmbase4layerfeat',\n",
       " 'rock.00052.wavlmbase4layerfeat',\n",
       " 'rock.00053.wavlmbase4layerfeat',\n",
       " 'rock.00054.wavlmbase4layerfeat',\n",
       " 'rock.00055.wavlmbase4layerfeat',\n",
       " 'rock.00056.wavlmbase4layerfeat',\n",
       " 'rock.00057.wavlmbase4layerfeat',\n",
       " 'rock.00058.wavlmbase4layerfeat',\n",
       " 'rock.00059.wavlmbase4layerfeat',\n",
       " 'rock.00060.wavlmbase4layerfeat',\n",
       " 'rock.00061.wavlmbase4layerfeat',\n",
       " 'rock.00062.wavlmbase4layerfeat',\n",
       " 'rock.00063.wavlmbase4layerfeat',\n",
       " 'rock.00064.wavlmbase4layerfeat',\n",
       " 'rock.00065.wavlmbase4layerfeat',\n",
       " 'rock.00066.wavlmbase4layerfeat',\n",
       " 'rock.00067.wavlmbase4layerfeat',\n",
       " 'rock.00068.wavlmbase4layerfeat',\n",
       " 'rock.00069.wavlmbase4layerfeat',\n",
       " 'rock.00070.wavlmbase4layerfeat',\n",
       " 'rock.00071.wavlmbase4layerfeat',\n",
       " 'rock.00072.wavlmbase4layerfeat',\n",
       " 'rock.00073.wavlmbase4layerfeat',\n",
       " 'rock.00074.wavlmbase4layerfeat',\n",
       " 'rock.00075.wavlmbase4layerfeat',\n",
       " 'rock.00076.wavlmbase4layerfeat',\n",
       " 'rock.00077.wavlmbase4layerfeat',\n",
       " 'rock.00078.wavlmbase4layerfeat',\n",
       " 'rock.00079.wavlmbase4layerfeat',\n",
       " 'rock.00080.wavlmbase4layerfeat',\n",
       " 'rock.00081.wavlmbase4layerfeat',\n",
       " 'rock.00082.wavlmbase4layerfeat',\n",
       " 'rock.00083.wavlmbase4layerfeat',\n",
       " 'rock.00084.wavlmbase4layerfeat',\n",
       " 'rock.00085.wavlmbase4layerfeat',\n",
       " 'rock.00086.wavlmbase4layerfeat',\n",
       " 'rock.00087.wavlmbase4layerfeat',\n",
       " 'rock.00088.wavlmbase4layerfeat',\n",
       " 'rock.00089.wavlmbase4layerfeat',\n",
       " 'rock.00090.wavlmbase4layerfeat',\n",
       " 'rock.00091.wavlmbase4layerfeat',\n",
       " 'rock.00092.wavlmbase4layerfeat',\n",
       " 'rock.00093.wavlmbase4layerfeat',\n",
       " 'rock.00094.wavlmbase4layerfeat',\n",
       " 'rock.00095.wavlmbase4layerfeat',\n",
       " 'rock.00096.wavlmbase4layerfeat',\n",
       " 'rock.00097.wavlmbase4layerfeat',\n",
       " 'rock.00098.wavlmbase4layerfeat',\n",
       " 'rock.00099.wavlmbase4layerfeat']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to feature files\n",
    "path_train = 'features'\n",
    "extension = 'wavlmbase4layerfeat'\n",
    "\n",
    "train_files = [file for file in os.listdir(path_train) if file.endswith(extension)]\n",
    "\n",
    "sorted_train_files = sorted(train_files)\n",
    "sorted_train_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e4b92-b653-4950-af31-d82dc1a08e84",
   "metadata": {},
   "source": [
    "### All feature vectors into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5599ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.204498</td>\n",
       "      <td>-0.093031</td>\n",
       "      <td>-0.039956</td>\n",
       "      <td>-0.562223</td>\n",
       "      <td>0.138806</td>\n",
       "      <td>0.096212</td>\n",
       "      <td>0.176993</td>\n",
       "      <td>-0.055592</td>\n",
       "      <td>-0.219515</td>\n",
       "      <td>-0.034371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032864</td>\n",
       "      <td>0.273840</td>\n",
       "      <td>0.043254</td>\n",
       "      <td>-0.051994</td>\n",
       "      <td>-0.009695</td>\n",
       "      <td>0.203010</td>\n",
       "      <td>-0.145996</td>\n",
       "      <td>0.044121</td>\n",
       "      <td>0.116998</td>\n",
       "      <td>-0.454236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009433</td>\n",
       "      <td>-0.111695</td>\n",
       "      <td>0.138206</td>\n",
       "      <td>-0.308871</td>\n",
       "      <td>0.143759</td>\n",
       "      <td>0.035443</td>\n",
       "      <td>0.315600</td>\n",
       "      <td>-0.221085</td>\n",
       "      <td>-0.141687</td>\n",
       "      <td>-0.155910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049276</td>\n",
       "      <td>0.225967</td>\n",
       "      <td>0.035753</td>\n",
       "      <td>-0.500972</td>\n",
       "      <td>-0.127206</td>\n",
       "      <td>0.349124</td>\n",
       "      <td>-0.081059</td>\n",
       "      <td>-0.021214</td>\n",
       "      <td>0.157881</td>\n",
       "      <td>-0.250526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.212292</td>\n",
       "      <td>-0.142811</td>\n",
       "      <td>0.119149</td>\n",
       "      <td>-0.172413</td>\n",
       "      <td>0.323740</td>\n",
       "      <td>-0.006175</td>\n",
       "      <td>-0.136769</td>\n",
       "      <td>-0.127913</td>\n",
       "      <td>-0.247721</td>\n",
       "      <td>0.038505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243754</td>\n",
       "      <td>0.186927</td>\n",
       "      <td>-0.310384</td>\n",
       "      <td>-0.350090</td>\n",
       "      <td>-0.029887</td>\n",
       "      <td>-0.295870</td>\n",
       "      <td>-0.069211</td>\n",
       "      <td>0.012515</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>-0.539947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.217505</td>\n",
       "      <td>-0.207070</td>\n",
       "      <td>0.157117</td>\n",
       "      <td>-0.136493</td>\n",
       "      <td>0.099840</td>\n",
       "      <td>0.300491</td>\n",
       "      <td>-0.109374</td>\n",
       "      <td>-0.290099</td>\n",
       "      <td>0.096063</td>\n",
       "      <td>-0.049180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107468</td>\n",
       "      <td>0.170206</td>\n",
       "      <td>0.228354</td>\n",
       "      <td>-0.841270</td>\n",
       "      <td>-0.096904</td>\n",
       "      <td>0.145656</td>\n",
       "      <td>0.060646</td>\n",
       "      <td>-0.117495</td>\n",
       "      <td>0.356633</td>\n",
       "      <td>-0.684860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.067425</td>\n",
       "      <td>-0.095689</td>\n",
       "      <td>0.088243</td>\n",
       "      <td>0.198330</td>\n",
       "      <td>0.169520</td>\n",
       "      <td>0.297094</td>\n",
       "      <td>-0.380811</td>\n",
       "      <td>-0.165345</td>\n",
       "      <td>0.327136</td>\n",
       "      <td>0.081083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011918</td>\n",
       "      <td>-0.207443</td>\n",
       "      <td>-0.404519</td>\n",
       "      <td>-0.349134</td>\n",
       "      <td>0.058498</td>\n",
       "      <td>0.270632</td>\n",
       "      <td>-0.039725</td>\n",
       "      <td>-0.023145</td>\n",
       "      <td>0.088038</td>\n",
       "      <td>-0.565479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500430</th>\n",
       "      <td>-0.186611</td>\n",
       "      <td>-0.040473</td>\n",
       "      <td>-0.010707</td>\n",
       "      <td>-0.056597</td>\n",
       "      <td>-0.069689</td>\n",
       "      <td>-0.261930</td>\n",
       "      <td>-0.232433</td>\n",
       "      <td>-0.146833</td>\n",
       "      <td>-0.144910</td>\n",
       "      <td>0.074270</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045094</td>\n",
       "      <td>0.128217</td>\n",
       "      <td>0.129677</td>\n",
       "      <td>1.018336</td>\n",
       "      <td>-0.058187</td>\n",
       "      <td>0.269444</td>\n",
       "      <td>-0.158515</td>\n",
       "      <td>0.079553</td>\n",
       "      <td>-0.228222</td>\n",
       "      <td>1.265655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500431</th>\n",
       "      <td>-0.255262</td>\n",
       "      <td>-0.047176</td>\n",
       "      <td>-0.005718</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>-0.046345</td>\n",
       "      <td>-0.060902</td>\n",
       "      <td>-0.154236</td>\n",
       "      <td>-0.163157</td>\n",
       "      <td>-0.093274</td>\n",
       "      <td>-0.034746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053642</td>\n",
       "      <td>0.113619</td>\n",
       "      <td>0.097742</td>\n",
       "      <td>0.858851</td>\n",
       "      <td>-0.024167</td>\n",
       "      <td>0.147876</td>\n",
       "      <td>-0.091196</td>\n",
       "      <td>-0.156619</td>\n",
       "      <td>-0.238391</td>\n",
       "      <td>0.865757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500432</th>\n",
       "      <td>-0.177335</td>\n",
       "      <td>-0.050510</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>-0.002687</td>\n",
       "      <td>0.054164</td>\n",
       "      <td>-0.095137</td>\n",
       "      <td>-0.166793</td>\n",
       "      <td>-0.071056</td>\n",
       "      <td>0.063231</td>\n",
       "      <td>-0.136739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092437</td>\n",
       "      <td>0.108632</td>\n",
       "      <td>0.052940</td>\n",
       "      <td>0.289008</td>\n",
       "      <td>0.029921</td>\n",
       "      <td>0.091171</td>\n",
       "      <td>0.023713</td>\n",
       "      <td>-0.138581</td>\n",
       "      <td>-0.210309</td>\n",
       "      <td>1.180158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500433</th>\n",
       "      <td>-0.254128</td>\n",
       "      <td>-0.052435</td>\n",
       "      <td>-0.123181</td>\n",
       "      <td>-0.125148</td>\n",
       "      <td>-0.051198</td>\n",
       "      <td>-0.381556</td>\n",
       "      <td>-0.161859</td>\n",
       "      <td>-0.092975</td>\n",
       "      <td>-0.077006</td>\n",
       "      <td>-0.060956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022917</td>\n",
       "      <td>-0.114606</td>\n",
       "      <td>-0.354498</td>\n",
       "      <td>0.549883</td>\n",
       "      <td>-0.089569</td>\n",
       "      <td>0.157596</td>\n",
       "      <td>-0.081541</td>\n",
       "      <td>0.091145</td>\n",
       "      <td>-0.177990</td>\n",
       "      <td>1.068273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500434</th>\n",
       "      <td>-0.306804</td>\n",
       "      <td>-0.111736</td>\n",
       "      <td>-0.190589</td>\n",
       "      <td>-0.125642</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>-0.288340</td>\n",
       "      <td>-0.205276</td>\n",
       "      <td>-0.141136</td>\n",
       "      <td>-0.264398</td>\n",
       "      <td>-0.131147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063631</td>\n",
       "      <td>-0.161267</td>\n",
       "      <td>-0.506085</td>\n",
       "      <td>0.918364</td>\n",
       "      <td>-0.213196</td>\n",
       "      <td>0.029896</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>0.173139</td>\n",
       "      <td>-0.175665</td>\n",
       "      <td>0.683726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500435 rows  768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "0        0.204498 -0.093031 -0.039956 -0.562223  0.138806  0.096212  0.176993   \n",
       "1        0.009433 -0.111695  0.138206 -0.308871  0.143759  0.035443  0.315600   \n",
       "2       -0.212292 -0.142811  0.119149 -0.172413  0.323740 -0.006175 -0.136769   \n",
       "3       -0.217505 -0.207070  0.157117 -0.136493  0.099840  0.300491 -0.109374   \n",
       "4        0.067425 -0.095689  0.088243  0.198330  0.169520  0.297094 -0.380811   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1500430 -0.186611 -0.040473 -0.010707 -0.056597 -0.069689 -0.261930 -0.232433   \n",
       "1500431 -0.255262 -0.047176 -0.005718 -0.002291 -0.046345 -0.060902 -0.154236   \n",
       "1500432 -0.177335 -0.050510  0.046610 -0.002687  0.054164 -0.095137 -0.166793   \n",
       "1500433 -0.254128 -0.052435 -0.123181 -0.125148 -0.051198 -0.381556 -0.161859   \n",
       "1500434 -0.306804 -0.111736 -0.190589 -0.125642  0.004307 -0.288340 -0.205276   \n",
       "\n",
       "                7         8         9  ...       758       759       760  \\\n",
       "0       -0.055592 -0.219515 -0.034371  ...  0.032864  0.273840  0.043254   \n",
       "1       -0.221085 -0.141687 -0.155910  ...  0.049276  0.225967  0.035753   \n",
       "2       -0.127913 -0.247721  0.038505  ...  0.243754  0.186927 -0.310384   \n",
       "3       -0.290099  0.096063 -0.049180  ...  0.107468  0.170206  0.228354   \n",
       "4       -0.165345  0.327136  0.081083  ...  0.011918 -0.207443 -0.404519   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "1500430 -0.146833 -0.144910  0.074270  ... -0.045094  0.128217  0.129677   \n",
       "1500431 -0.163157 -0.093274 -0.034746  ...  0.053642  0.113619  0.097742   \n",
       "1500432 -0.071056  0.063231 -0.136739  ...  0.092437  0.108632  0.052940   \n",
       "1500433 -0.092975 -0.077006 -0.060956  ...  0.022917 -0.114606 -0.354498   \n",
       "1500434 -0.141136 -0.264398 -0.131147  ... -0.063631 -0.161267 -0.506085   \n",
       "\n",
       "              761       762       763       764       765       766       767  \n",
       "0       -0.051994 -0.009695  0.203010 -0.145996  0.044121  0.116998 -0.454236  \n",
       "1       -0.500972 -0.127206  0.349124 -0.081059 -0.021214  0.157881 -0.250526  \n",
       "2       -0.350090 -0.029887 -0.295870 -0.069211  0.012515  0.282609 -0.539947  \n",
       "3       -0.841270 -0.096904  0.145656  0.060646 -0.117495  0.356633 -0.684860  \n",
       "4       -0.349134  0.058498  0.270632 -0.039725 -0.023145  0.088038 -0.565479  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "1500430  1.018336 -0.058187  0.269444 -0.158515  0.079553 -0.228222  1.265655  \n",
       "1500431  0.858851 -0.024167  0.147876 -0.091196 -0.156619 -0.238391  0.865757  \n",
       "1500432  0.289008  0.029921  0.091171  0.023713 -0.138581 -0.210309  1.180158  \n",
       "1500433  0.549883 -0.089569  0.157596 -0.081541  0.091145 -0.177990  1.068273  \n",
       "1500434  0.918364 -0.213196  0.029896  0.014822  0.173139 -0.175665  0.683726  \n",
       "\n",
       "[1500435 rows x 768 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for file in sorted_train_files:\n",
    "    df = pd.read_csv(os.path.join(path_train, file))\n",
    "    dfs.append(df)\n",
    "\n",
    "df_train_feat = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop first index column (unamed 0)\n",
    "df_train_feat.drop(df_train_feat.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "df_train_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a43bb",
   "metadata": {},
   "source": [
    "### Process label files - Train and Devel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731cff8d-4698-4b12-be00-9f8df9812777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/etsmtl/akoerich/DEV/Music\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a5d308-c098-41d5-84a0-5494987b7bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   genre  track_number  label\n",
      "0  blues             0      0\n",
      "1  blues             0      0\n",
      "2  blues             0      0\n",
      "3  blues             0      0\n",
      "4  blues             0      0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the GTzan dataset\n",
    "gtzan_path = \"GTzan_16k_Wav\"\n",
    "gtzan_path_features = \"features\"\n",
    "\n",
    "# Dictionary to map genre names to numeric labels\n",
    "genre_label_map = {\n",
    "    \"blues\": 0,\n",
    "    \"classical\": 1,\n",
    "    \"country\": 2,\n",
    "    \"disco\": 3,\n",
    "    \"hiphop\": 4,\n",
    "    \"jazz\": 5,\n",
    "    \"metal\": 6,\n",
    "    \"pop\": 7,\n",
    "    \"reggae\": 8,\n",
    "    \"rock\": 9\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data_list = []\n",
    "\n",
    "# Iterate through the directories\n",
    "for genre in sorted( os.listdir(gtzan_path) ):\n",
    "    genre_path = os.path.join(gtzan_path, genre)\n",
    "    if os.path.isdir(genre_path) :\n",
    "        for filename in sorted( os.listdir(genre_path) ):\n",
    "            #print(filename)\n",
    "            # Extract genre, track number, and extension from filename\n",
    "            genre_track, extension = os.path.splitext(filename)\n",
    "            # Split the genre_track into genre and track number\n",
    "            genre, track_number, au = genre_track.split('.')\n",
    "            # Ensure it's a wav file\n",
    "            if extension == \".wav\":\n",
    "                # Count number of lines in the corresponding feature file\n",
    "                feature_filename = f\"{genre}.{track_number}.wavlmbase4layerfeat\"\n",
    "                # feature_filepath = os.path.join(genre_path, feature_filename)\n",
    "                feature_filepath = os.path.join(gtzan_path_features, feature_filename)\n",
    "                if os.path.exists(feature_filepath):\n",
    "                    with open(feature_filepath, 'r') as f:\n",
    "                        num_lines = sum(1 for line in f)\n",
    "                        num_lines = num_lines - 1\n",
    "                    # Append a dictionary to the list replicated by the number of lines\n",
    "                    for _ in range(num_lines):\n",
    "                        data_list.append({\n",
    "                            \"genre\": genre,\n",
    "                            \"track_number\": int(track_number),\n",
    "                            \"label\": genre_label_map[genre]\n",
    "                        })\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df_train_lab = pd.DataFrame(data_list)\n",
    "\n",
    "# Print first few rows of the DataFrame\n",
    "print(df_train_lab.head())\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df_train_lab.to_csv(\"gtzan_dataset_inflated_wavlmbase4layerfeat.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f87cb6df-8741-4c05-a2a3-34c9cd49c056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.204498</td>\n",
       "      <td>-0.093031</td>\n",
       "      <td>-0.039956</td>\n",
       "      <td>-0.562223</td>\n",
       "      <td>0.138806</td>\n",
       "      <td>0.096212</td>\n",
       "      <td>0.176993</td>\n",
       "      <td>-0.055592</td>\n",
       "      <td>-0.219515</td>\n",
       "      <td>-0.034371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032864</td>\n",
       "      <td>0.273840</td>\n",
       "      <td>0.043254</td>\n",
       "      <td>-0.051994</td>\n",
       "      <td>-0.009695</td>\n",
       "      <td>0.203010</td>\n",
       "      <td>-0.145996</td>\n",
       "      <td>0.044121</td>\n",
       "      <td>0.116998</td>\n",
       "      <td>-0.454236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009433</td>\n",
       "      <td>-0.111695</td>\n",
       "      <td>0.138206</td>\n",
       "      <td>-0.308871</td>\n",
       "      <td>0.143759</td>\n",
       "      <td>0.035443</td>\n",
       "      <td>0.315600</td>\n",
       "      <td>-0.221085</td>\n",
       "      <td>-0.141687</td>\n",
       "      <td>-0.155910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049276</td>\n",
       "      <td>0.225967</td>\n",
       "      <td>0.035753</td>\n",
       "      <td>-0.500972</td>\n",
       "      <td>-0.127206</td>\n",
       "      <td>0.349124</td>\n",
       "      <td>-0.081059</td>\n",
       "      <td>-0.021214</td>\n",
       "      <td>0.157881</td>\n",
       "      <td>-0.250526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.212292</td>\n",
       "      <td>-0.142811</td>\n",
       "      <td>0.119149</td>\n",
       "      <td>-0.172413</td>\n",
       "      <td>0.323740</td>\n",
       "      <td>-0.006175</td>\n",
       "      <td>-0.136769</td>\n",
       "      <td>-0.127913</td>\n",
       "      <td>-0.247721</td>\n",
       "      <td>0.038505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243754</td>\n",
       "      <td>0.186927</td>\n",
       "      <td>-0.310384</td>\n",
       "      <td>-0.350090</td>\n",
       "      <td>-0.029887</td>\n",
       "      <td>-0.295870</td>\n",
       "      <td>-0.069211</td>\n",
       "      <td>0.012515</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>-0.539947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.217505</td>\n",
       "      <td>-0.207070</td>\n",
       "      <td>0.157117</td>\n",
       "      <td>-0.136493</td>\n",
       "      <td>0.099840</td>\n",
       "      <td>0.300491</td>\n",
       "      <td>-0.109374</td>\n",
       "      <td>-0.290099</td>\n",
       "      <td>0.096063</td>\n",
       "      <td>-0.049180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107468</td>\n",
       "      <td>0.170206</td>\n",
       "      <td>0.228354</td>\n",
       "      <td>-0.841270</td>\n",
       "      <td>-0.096904</td>\n",
       "      <td>0.145656</td>\n",
       "      <td>0.060646</td>\n",
       "      <td>-0.117495</td>\n",
       "      <td>0.356633</td>\n",
       "      <td>-0.684860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.067425</td>\n",
       "      <td>-0.095689</td>\n",
       "      <td>0.088243</td>\n",
       "      <td>0.198330</td>\n",
       "      <td>0.169520</td>\n",
       "      <td>0.297094</td>\n",
       "      <td>-0.380811</td>\n",
       "      <td>-0.165345</td>\n",
       "      <td>0.327136</td>\n",
       "      <td>0.081083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011918</td>\n",
       "      <td>-0.207443</td>\n",
       "      <td>-0.404519</td>\n",
       "      <td>-0.349134</td>\n",
       "      <td>0.058498</td>\n",
       "      <td>0.270632</td>\n",
       "      <td>-0.039725</td>\n",
       "      <td>-0.023145</td>\n",
       "      <td>0.088038</td>\n",
       "      <td>-0.565479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500430</th>\n",
       "      <td>-0.186611</td>\n",
       "      <td>-0.040473</td>\n",
       "      <td>-0.010707</td>\n",
       "      <td>-0.056597</td>\n",
       "      <td>-0.069689</td>\n",
       "      <td>-0.261930</td>\n",
       "      <td>-0.232433</td>\n",
       "      <td>-0.146833</td>\n",
       "      <td>-0.144910</td>\n",
       "      <td>0.074270</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045094</td>\n",
       "      <td>0.128217</td>\n",
       "      <td>0.129677</td>\n",
       "      <td>1.018336</td>\n",
       "      <td>-0.058187</td>\n",
       "      <td>0.269444</td>\n",
       "      <td>-0.158515</td>\n",
       "      <td>0.079553</td>\n",
       "      <td>-0.228222</td>\n",
       "      <td>1.265655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500431</th>\n",
       "      <td>-0.255262</td>\n",
       "      <td>-0.047176</td>\n",
       "      <td>-0.005718</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>-0.046345</td>\n",
       "      <td>-0.060902</td>\n",
       "      <td>-0.154236</td>\n",
       "      <td>-0.163157</td>\n",
       "      <td>-0.093274</td>\n",
       "      <td>-0.034746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053642</td>\n",
       "      <td>0.113619</td>\n",
       "      <td>0.097742</td>\n",
       "      <td>0.858851</td>\n",
       "      <td>-0.024167</td>\n",
       "      <td>0.147876</td>\n",
       "      <td>-0.091196</td>\n",
       "      <td>-0.156619</td>\n",
       "      <td>-0.238391</td>\n",
       "      <td>0.865757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500432</th>\n",
       "      <td>-0.177335</td>\n",
       "      <td>-0.050510</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>-0.002687</td>\n",
       "      <td>0.054164</td>\n",
       "      <td>-0.095137</td>\n",
       "      <td>-0.166793</td>\n",
       "      <td>-0.071056</td>\n",
       "      <td>0.063231</td>\n",
       "      <td>-0.136739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092437</td>\n",
       "      <td>0.108632</td>\n",
       "      <td>0.052940</td>\n",
       "      <td>0.289008</td>\n",
       "      <td>0.029921</td>\n",
       "      <td>0.091171</td>\n",
       "      <td>0.023713</td>\n",
       "      <td>-0.138581</td>\n",
       "      <td>-0.210309</td>\n",
       "      <td>1.180158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500433</th>\n",
       "      <td>-0.254128</td>\n",
       "      <td>-0.052435</td>\n",
       "      <td>-0.123181</td>\n",
       "      <td>-0.125148</td>\n",
       "      <td>-0.051198</td>\n",
       "      <td>-0.381556</td>\n",
       "      <td>-0.161859</td>\n",
       "      <td>-0.092975</td>\n",
       "      <td>-0.077006</td>\n",
       "      <td>-0.060956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022917</td>\n",
       "      <td>-0.114606</td>\n",
       "      <td>-0.354498</td>\n",
       "      <td>0.549883</td>\n",
       "      <td>-0.089569</td>\n",
       "      <td>0.157596</td>\n",
       "      <td>-0.081541</td>\n",
       "      <td>0.091145</td>\n",
       "      <td>-0.177990</td>\n",
       "      <td>1.068273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500434</th>\n",
       "      <td>-0.306804</td>\n",
       "      <td>-0.111736</td>\n",
       "      <td>-0.190589</td>\n",
       "      <td>-0.125642</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>-0.288340</td>\n",
       "      <td>-0.205276</td>\n",
       "      <td>-0.141136</td>\n",
       "      <td>-0.264398</td>\n",
       "      <td>-0.131147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063631</td>\n",
       "      <td>-0.161267</td>\n",
       "      <td>-0.506085</td>\n",
       "      <td>0.918364</td>\n",
       "      <td>-0.213196</td>\n",
       "      <td>0.029896</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>0.173139</td>\n",
       "      <td>-0.175665</td>\n",
       "      <td>0.683726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500435 rows  768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "0        0.204498 -0.093031 -0.039956 -0.562223  0.138806  0.096212  0.176993   \n",
       "1        0.009433 -0.111695  0.138206 -0.308871  0.143759  0.035443  0.315600   \n",
       "2       -0.212292 -0.142811  0.119149 -0.172413  0.323740 -0.006175 -0.136769   \n",
       "3       -0.217505 -0.207070  0.157117 -0.136493  0.099840  0.300491 -0.109374   \n",
       "4        0.067425 -0.095689  0.088243  0.198330  0.169520  0.297094 -0.380811   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1500430 -0.186611 -0.040473 -0.010707 -0.056597 -0.069689 -0.261930 -0.232433   \n",
       "1500431 -0.255262 -0.047176 -0.005718 -0.002291 -0.046345 -0.060902 -0.154236   \n",
       "1500432 -0.177335 -0.050510  0.046610 -0.002687  0.054164 -0.095137 -0.166793   \n",
       "1500433 -0.254128 -0.052435 -0.123181 -0.125148 -0.051198 -0.381556 -0.161859   \n",
       "1500434 -0.306804 -0.111736 -0.190589 -0.125642  0.004307 -0.288340 -0.205276   \n",
       "\n",
       "                7         8         9  ...       758       759       760  \\\n",
       "0       -0.055592 -0.219515 -0.034371  ...  0.032864  0.273840  0.043254   \n",
       "1       -0.221085 -0.141687 -0.155910  ...  0.049276  0.225967  0.035753   \n",
       "2       -0.127913 -0.247721  0.038505  ...  0.243754  0.186927 -0.310384   \n",
       "3       -0.290099  0.096063 -0.049180  ...  0.107468  0.170206  0.228354   \n",
       "4       -0.165345  0.327136  0.081083  ...  0.011918 -0.207443 -0.404519   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "1500430 -0.146833 -0.144910  0.074270  ... -0.045094  0.128217  0.129677   \n",
       "1500431 -0.163157 -0.093274 -0.034746  ...  0.053642  0.113619  0.097742   \n",
       "1500432 -0.071056  0.063231 -0.136739  ...  0.092437  0.108632  0.052940   \n",
       "1500433 -0.092975 -0.077006 -0.060956  ...  0.022917 -0.114606 -0.354498   \n",
       "1500434 -0.141136 -0.264398 -0.131147  ... -0.063631 -0.161267 -0.506085   \n",
       "\n",
       "              761       762       763       764       765       766       767  \n",
       "0       -0.051994 -0.009695  0.203010 -0.145996  0.044121  0.116998 -0.454236  \n",
       "1       -0.500972 -0.127206  0.349124 -0.081059 -0.021214  0.157881 -0.250526  \n",
       "2       -0.350090 -0.029887 -0.295870 -0.069211  0.012515  0.282609 -0.539947  \n",
       "3       -0.841270 -0.096904  0.145656  0.060646 -0.117495  0.356633 -0.684860  \n",
       "4       -0.349134  0.058498  0.270632 -0.039725 -0.023145  0.088038 -0.565479  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "1500430  1.018336 -0.058187  0.269444 -0.158515  0.079553 -0.228222  1.265655  \n",
       "1500431  0.858851 -0.024167  0.147876 -0.091196 -0.156619 -0.238391  0.865757  \n",
       "1500432  0.289008  0.029921  0.091171  0.023713 -0.138581 -0.210309  1.180158  \n",
       "1500433  0.549883 -0.089569  0.157596 -0.081541  0.091145 -0.177990  1.068273  \n",
       "1500434  0.918364 -0.213196  0.029896  0.014822  0.173139 -0.175665  0.683726  \n",
       "\n",
       "[1500435 rows x 768 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572759e",
   "metadata": {},
   "source": [
    "## Train a GRU regression model for arousal / valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a681dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a081309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2814445c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 1.7597\n",
      "Epoch [1/1000], Validation Loss: 1.7378\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [2/1000], Loss: 1.6999\n",
      "Epoch [3/1000], Loss: 1.6563\n",
      "Epoch [3/1000], Validation Loss: 1.6227\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [4/1000], Loss: 1.6343\n",
      "Epoch [5/1000], Loss: 1.6101\n",
      "Epoch [5/1000], Validation Loss: 1.5962\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [6/1000], Loss: 1.6268\n",
      "Epoch [7/1000], Loss: 1.5979\n",
      "Epoch [7/1000], Validation Loss: 1.5782\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [8/1000], Loss: 1.5935\n",
      "Epoch [9/1000], Loss: 1.5918\n",
      "Epoch [9/1000], Validation Loss: 1.5658\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [10/1000], Loss: 1.5923\n",
      "Epoch [11/1000], Loss: 1.5663\n",
      "Epoch [11/1000], Validation Loss: 1.5574\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [12/1000], Loss: 1.5835\n",
      "Epoch [13/1000], Loss: 1.5676\n",
      "Epoch [13/1000], Validation Loss: 1.5496\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [14/1000], Loss: 1.5625\n",
      "Epoch [15/1000], Loss: 1.5553\n",
      "Epoch [15/1000], Validation Loss: 1.5426\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [16/1000], Loss: 1.5724\n",
      "Epoch [17/1000], Loss: 1.5565\n",
      "Epoch [17/1000], Validation Loss: 1.5369\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [18/1000], Loss: 1.5612\n",
      "Epoch [19/1000], Loss: 1.5391\n",
      "Epoch [19/1000], Validation Loss: 1.5312\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [20/1000], Loss: 1.5585\n",
      "Epoch [21/1000], Loss: 1.5426\n",
      "Epoch [21/1000], Validation Loss: 1.5279\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [22/1000], Loss: 1.5454\n",
      "Epoch [23/1000], Loss: 1.5544\n",
      "Epoch [23/1000], Validation Loss: 1.5258\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [24/1000], Loss: 1.5452\n",
      "Epoch [25/1000], Loss: 1.5373\n",
      "Epoch [25/1000], Validation Loss: 1.5234\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [26/1000], Loss: 1.5391\n",
      "Epoch [27/1000], Loss: 1.5287\n",
      "Epoch [27/1000], Validation Loss: 1.5212\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [28/1000], Loss: 1.5311\n",
      "Epoch [29/1000], Loss: 1.5586\n",
      "Epoch [29/1000], Validation Loss: 1.5189\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [30/1000], Loss: 1.5128\n",
      "Epoch [31/1000], Loss: 1.5358\n",
      "Epoch [31/1000], Validation Loss: 1.5166\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [32/1000], Loss: 1.5427\n",
      "Epoch [33/1000], Loss: 1.5371\n",
      "Epoch [33/1000], Validation Loss: 1.5160\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [34/1000], Loss: 1.5206\n",
      "Epoch [35/1000], Loss: 1.5220\n",
      "Epoch [35/1000], Validation Loss: 1.5125\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [36/1000], Loss: 1.5266\n",
      "Epoch [37/1000], Loss: 1.5343\n",
      "Epoch [37/1000], Validation Loss: 1.5116\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [38/1000], Loss: 1.5158\n",
      "Epoch [39/1000], Loss: 1.5074\n",
      "Epoch [39/1000], Validation Loss: 1.5109\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [40/1000], Loss: 1.5231\n",
      "Epoch [41/1000], Loss: 1.5221\n",
      "Epoch [41/1000], Validation Loss: 1.5104\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [42/1000], Loss: 1.5364\n",
      "Epoch [43/1000], Loss: 1.5103\n",
      "Epoch [43/1000], Validation Loss: 1.5089\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [44/1000], Loss: 1.5185\n",
      "Epoch [45/1000], Loss: 1.5080\n",
      "Epoch [45/1000], Validation Loss: 1.5063\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [46/1000], Loss: 1.5227\n",
      "Epoch [47/1000], Loss: 1.5098\n",
      "Epoch [47/1000], Validation Loss: 1.5064\n",
      "Epoch [48/1000], Loss: 1.5160\n",
      "Epoch [49/1000], Loss: 1.5019\n",
      "Epoch [49/1000], Validation Loss: 1.5059\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [50/1000], Loss: 1.5047\n",
      "Epoch [51/1000], Loss: 1.5204\n",
      "Epoch [51/1000], Validation Loss: 1.5045\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [52/1000], Loss: 1.5118\n",
      "Epoch [53/1000], Loss: 1.5064\n",
      "Epoch [53/1000], Validation Loss: 1.5032\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [54/1000], Loss: 1.5102\n",
      "Epoch [55/1000], Loss: 1.5132\n",
      "Epoch [55/1000], Validation Loss: 1.5033\n",
      "Epoch [56/1000], Loss: 1.5073\n",
      "Epoch [57/1000], Loss: 1.5151\n",
      "Epoch [57/1000], Validation Loss: 1.5042\n",
      "Epoch [58/1000], Loss: 1.5095\n",
      "Epoch [59/1000], Loss: 1.4936\n",
      "Epoch [59/1000], Validation Loss: 1.5019\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [60/1000], Loss: 1.5132\n",
      "Epoch [61/1000], Loss: 1.5174\n",
      "Epoch [61/1000], Validation Loss: 1.5006\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [62/1000], Loss: 1.5087\n",
      "Epoch [63/1000], Loss: 1.5023\n",
      "Epoch [63/1000], Validation Loss: 1.5023\n",
      "Epoch [64/1000], Loss: 1.5062\n",
      "Epoch [65/1000], Loss: 1.5126\n",
      "Epoch [65/1000], Validation Loss: 1.5012\n",
      "Epoch [66/1000], Loss: 1.5125\n",
      "Epoch [67/1000], Loss: 1.5216\n",
      "Epoch [67/1000], Validation Loss: 1.5021\n",
      "Epoch [68/1000], Loss: 1.4986\n",
      "Epoch [69/1000], Loss: 1.5106\n",
      "Epoch [69/1000], Validation Loss: 1.5010\n",
      "Epoch [70/1000], Loss: 1.4977\n",
      "Epoch [71/1000], Loss: 1.5166\n",
      "Epoch [71/1000], Validation Loss: 1.4990\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n",
      "Epoch [72/1000], Loss: 1.5305\n",
      "Epoch [73/1000], Loss: 1.5081\n",
      "Epoch [73/1000], Validation Loss: 1.4986\n",
      "Saved model with best validation loss to best_model_gtzan.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 181\u001b[0m\n\u001b[1;32m    177\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    179\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    182\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    183\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch_X\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/anaconda3/envs/wavlm/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/wavlm/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/wavlm/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/wavlm/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/wavlm/lib/python3.11/site-packages/torch/utils/data/dataset.py:206\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/wavlm/lib/python3.11/site-packages/torch/utils/data/dataset.py:206\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    StepLR, ReduceLROnPlateau, MultiStepLR, ExponentialLR, CosineAnnealingLR\n",
    ")\n",
    "\n",
    "features = df_train_feat.values.astype(np.float32)\n",
    "labels   = df_train_lab['label'].values.astype(np.float32)\n",
    "\n",
    "# Normalize the features between -1 and 1 (adjust scaling based on your data)\n",
    "# features = (features - np.min(features)) / (np.max(features) - np.min(features)) * 2 - 1\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.from_numpy(features)\n",
    "labels_tensor   = torch.from_numpy(labels)\n",
    "\n",
    "# Assuming you want a sequence length of 1\n",
    "# features_tensor = features_tensor.unsqueeze(1)\n",
    "\n",
    "######\n",
    "# Reshape features tensor with sequence length of 50\n",
    "sequence_length = 1\n",
    "num_features    = features.shape[1]\n",
    "num_samples     = features.shape[0]\n",
    "\n",
    "# Calculate the number of sequences that can be formed\n",
    "num_sequences = num_samples // sequence_length\n",
    "\n",
    "# Truncate the tensor to fit the full sequences\n",
    "features_tensor = features_tensor[:num_sequences * sequence_length, :]\n",
    "labels_tensor = labels_tensor[:num_sequences * sequence_length]\n",
    "\n",
    "# Reshape the tensor\n",
    "features_tensor = features_tensor.view(num_sequences, sequence_length, num_features)\n",
    "\n",
    "######\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_tensor, labels_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size   = num_features\n",
    "hidden_size  = 64 #128, 64, 32, 16\n",
    "num_layers   = 4\n",
    "#output_size  = 1  # Single output for regression between -1 and +1\n",
    "num_classes  = 10 # GTzan\n",
    "dropout_prob = 0.20 \n",
    "# ======================\n",
    "# Define the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, dropout_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "        return output\n",
    "\n",
    "# model = GRUModel(input_size, hidden_size, output_size, dropout_prob)\n",
    "\n",
    "\n",
    "# =======================\n",
    "class GRUModelClassification(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
    "        \n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        # Apply softmax activation\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out\n",
    "        \n",
    "# =======================\n",
    "# Define the Convolutional GRU model\n",
    "class ConvGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(ConvGRUModel, self).__init__()\n",
    "        \n",
    "        # GRU layer\n",
    "        self.convgru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_prob, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        # h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device) \n",
    "\n",
    "        # Forward propagate GRU\n",
    "        #gru_out, _ = self.convgru(x, h0)\n",
    "        gru_out, _ = self.convgru(x)\n",
    "\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "\n",
    "        # Apply softmax activation\n",
    "        output = F.softmax(output, dim=1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "#model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "model = ConvGRUModel(input_size, hidden_size, num_layers, num_classes, dropout_prob)\n",
    "\n",
    "#=============================\n",
    "# Define the Convolutional GRU model with Tanh activation at the output\n",
    "class ConvGRUModelTanh(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(ConvGRUModelTanh, self).__init__()\n",
    "        self.convgru = nn.GRU(input_size=input_size, hidden_size=hidden_size, dropout=dropout_prob, num_layers=num_layers, batch_first=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.convgru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])  # Take the output from the last time step\n",
    "        output = self.tanh(output)  # Apply Tanh activation\n",
    "        return output\n",
    "\n",
    "#model = ConvGRUModelTanh(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "#=============================\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Define your loss function (criterion)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce lr by 10% every 10 epochs\n",
    "\n",
    "# Train the model\n",
    "num_epochs     = 1000\n",
    "batch_size     = 1500\n",
    "validate_every = 2  # Validate every 2 epochs\n",
    "patience       = 20  # Stop training if validation loss doesn't improve for 5 consecutive validations\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train.to(torch.int64))\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize a list to store the training loss values\n",
    "train_loss_values = []\n",
    "validation_loss_values = []\n",
    "\n",
    "best_validation_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "best_model_path = 'best_model_gtzan.pth'  # Define the path to save the best model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X.to(device))\n",
    "        loss = criterion(outputs, batch_y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update learning rate after each epoch (StepLR example)\n",
    "        # scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    average_epoch_loss = epoch_loss / len(train_loader)\n",
    "    train_loss_values.append(average_epoch_loss)\n",
    "    \n",
    "    # Validate the model every validate_every epochs using the test partition\n",
    "    if epoch % validate_every == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test.to(device))\n",
    "            validation_loss = criterion(test_outputs, y_test.to(torch.int64).to(device))  # Adjust target size\n",
    "\n",
    "        validation_loss_values.append(validation_loss.item())\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {validation_loss.item():.4f}')\n",
    "        \n",
    "        if validation_loss < best_validation_loss:\n",
    "            best_validation_loss = validation_loss\n",
    "            early_stop_counter = 0\n",
    "            \n",
    "            # Save the model with the best validation loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f'Saved model with best validation loss to {best_model_path}')\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1} as validation loss has not improved for {patience} consecutive validations.')\n",
    "            break\n",
    "            \n",
    "        model.train()  # Set the model back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# Load the best model for testing\n",
    "#best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, num_classes, dropout_prob)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.to(device)\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(X_test.to(device))\n",
    "    #test_loss    = criterion(test_outputs, y_test.unsqueeze(1).to(device))\n",
    "    test_loss    = criterion(test_outputs, y_test.to(torch.int64).to(device))\n",
    "\n",
    "print(f'Test Loss: {test_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612789be",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851ef95-fba3-4cad-a672-477a95fe5e93",
   "metadata": {},
   "source": [
    "### Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeadeff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the training loss values\n",
    "plt.plot(train_loss_values, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss values\n",
    "epochs = range(1, len(train_loss_values) + 1)\n",
    "plt.plot(epochs, train_loss_values, label='Training Loss')\n",
    "plt.plot(range(0, len(validation_loss_values) * validate_every, validate_every), validation_loss_values, label='Validation Loss', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3317ec0-f413-4a17-8c1a-ced5e816ee05",
   "metadata": {},
   "source": [
    "### Load Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604ab31-b494-4504-89fc-6b202e1eb08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = df_train_feat.values.astype(np.float32)\n",
    "labels2   = df_train_lab['label'].values.astype(np.float32)\n",
    "\n",
    "# Normalize the features between -1 and 1 (adjust scaling based on your data)\n",
    "# features = (features - np.min(features)) / (np.max(features) - np.min(features)) * 2 - 1\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor2 = torch.from_numpy(features2)\n",
    "labels_tensor2   = torch.from_numpy(labels2)\n",
    "\n",
    "# Assuming you want a sequence length of 1\n",
    "# features_tensor = features_tensor.unsqueeze(1)\n",
    "\n",
    "######\n",
    "# Reshape features tensor with sequence length of 50\n",
    "sequence_length = 1\n",
    "num_features    = features.shape[1]\n",
    "num_samples     = features.shape[0]\n",
    "\n",
    "# Calculate the number of sequences that can be formed\n",
    "num_sequences = num_samples // sequence_length\n",
    "\n",
    "# Truncate the tensor to fit the full sequences\n",
    "features_tensor2 = features_tensor2[:num_sequences * sequence_length, :]\n",
    "labels_tensor2   = labels_tensor2[:num_sequences * sequence_length]\n",
    "\n",
    "# Reshape the tensor\n",
    "features_tensor2 = features_tensor2.view(num_sequences, sequence_length, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b23f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tensor2.size()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f62054fb-0ce6-479e-8c2a-c30266cfdd2b",
   "metadata": {},
   "source": [
    "# Load the best model for testing\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(features_tensor2.to(device))\n",
    "    test_loss    = criterion(test_outputs, labels_tensor2.unsqueeze(1).to(device))\n",
    "\n",
    "print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84528c98-3fe5-405b-9dce-fb0ebc557366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# Load the best model for testing\n",
    "best_model = ConvGRUModel(input_size, hidden_size, num_layers, num_classes, dropout_prob)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.to(device)\n",
    "\n",
    "# A verificar\n",
    "#best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(features_tensor2.to(device))\n",
    "    test_loss    = criterion(test_outputs, labels_tensor2.to(torch.int64).to(device))\n",
    "\n",
    "print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e9507-9551-437d-b0f5-2867785f951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9bd7b-1964-4c6a-9b3e-15e78766ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = test_outputs.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a54420-6826-4d74-94af-c4b35dd76ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6379f1-d50e-4b3b-9633-d986405e909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f3664-f96c-446e-b744-aea9108b821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_label(predictions):\n",
    "  \"\"\"\n",
    "  Converts a 10-dimensional probability vector to the class label (index of max probability).\n",
    "\n",
    "  Args:\n",
    "      predictions: A torch.Tensor of size (batch_size, 10) containing probability predictions.\n",
    "\n",
    "  Returns:\n",
    "      A torch.Tensor of size (batch_size) containing the predicted class labels (integers).\n",
    "  \"\"\"\n",
    "  # Get the index of the maximum probability along the dimension with 10 elements (classes)\n",
    "  _, predicted_labels = torch.max(predictions, dim=1)\n",
    "  return predicted_labels\n",
    "\n",
    "# Example usage\n",
    "predict_y_int = predict_label(predict_y)\n",
    "print(\"Predicted labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a977a-a482-4f06-852a-5c47a7171a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audmetric\n",
    "\n",
    "print(\"Accuracy = \" + str(audmetric.accuracy(df_train_lab['label'], predict_y_int)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
