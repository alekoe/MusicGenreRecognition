{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "# End to end CNN for GTzan music classification EnvCNN\n",
    "\n",
    "\n",
    "WINDOWED Version\n",
    "\n",
    "Adapted by AL Koerich\n",
    "\n",
    "To GTzan 10-fold\n",
    "\n",
    "29 October 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import soundfile as sf\n",
    "from numpy import *\n",
    "import soundfile as sf\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "from keras import regularizers\n",
    "\n",
    "import os, sys\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, Dense, MaxPool1D, Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "import keras.initializers as init\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( )\n",
    "config.gpu_options.allow_growth = True\n",
    "sess   = tf.Session(config=config)\n",
    "import keras.backend.tensorflow_backend as tf_bkend\n",
    "tf_bkend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#controling_Hyper parameters\n",
    "batch_size = 10\n",
    "nb_classes = 10\n",
    "nb_epoch   = 300\n",
    "frame_size = 110250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load( \"/home-2/akoerich/GTzan_X_fold2-10_110250_75_256.npy\" )\n",
    "Y_train = np.load( \"/home-2/akoerich/GTzan_Y_fold2-10_110250_75_256.npy\" )\n",
    "# t_train = np.load( \"t_train_13_220500_50_256_box_id.npy\" )\n",
    "# s_train = np.load( \"s_train_13_220500_50_256_box_id.npy\" )\n",
    "\n",
    "\n",
    "X_valid = np.load( \"/home-2/akoerich/GTzan_X_fold1_110250_75_256.npy\" )\n",
    "Y_valid = np.load( \"/home-2/akoerich/GTzan_Y_fold1_110250_75_256.npy\" )\n",
    "# t_valid = np.load( \"t_valid_2_220500_50_256_box_id.npy\" )\n",
    "# s_valid = np.load( \"s_valid_2_220500_50_256_box_id.npy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-256.0, 256.0000000000001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.min(), X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-256.0, 256.0000000000001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.min(), X_valid.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a validation split is not provided\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_valid, Y_train, Y_valid = train_test_split(X_train2, Y_train2, test_size=0.10, stratify=Y_train2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#X_train2 = None\n",
    "#Y_train2 = None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22490, 110250, 1), (22490, 10), (2500, 110250, 1), (2500, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape\n",
    "#, t_train.shape, s_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_generator_soundnet():\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.models import Model\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "    inp =  Input(shape=(frame_size,1))\n",
    "    #----------------------\n",
    "    conv1 = Conv1D(filters=8, kernel_size=256, strides=2, activation='relu')(inp)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPool1D(pool_size=8, strides=8)(norm1)\n",
    "    #----------------------\n",
    "    conv2 = Conv1D(filters=16, kernel_size=128, strides=2, activation='relu')(pool1)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPool1D(pool_size=8, strides=8)(norm2)\n",
    "    #----------------------\n",
    "    conv3 = Conv1D(filters=32, kernel_size=64, strides=2, activation='relu')(pool2)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    #----------------------\n",
    "    conv4 = Conv1D(filters=64, kernel_size=32, strides=2, activation='relu')(norm3)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    #----------------------\n",
    "    conv5 = Conv1D(filters=128, kernel_size=16, strides=2, activation='relu')(norm4)\n",
    "    norm5 = BatchNormalization()(conv5)\n",
    "    pool3 = MaxPool1D(pool_size=4, strides=4)(norm5)\n",
    "\n",
    "    flat = Flatten()(pool3)\n",
    "\n",
    "    dense1 = Dense(1024, activation='relu')(flat)\n",
    "    drop1  = Dropout(0.50)(dense1)\n",
    "    \n",
    "    # dense1 = Dense(2048, activation='relu')(flat)\n",
    "    # drop1  = Dropout(0.50)(dense1)\n",
    "\n",
    "    # dense2 = Dense(1024, activation='relu')(dense1)\n",
    "    # drop2  = Dropout(0.50)(dense2)\n",
    "\n",
    "    dense2a = Dense(512, activation='relu')(drop1)\n",
    "    drop2a  = Dropout(0.50)(dense2a)\n",
    "    \n",
    "    dense3a = Dense(256, activation='relu')(drop2a)\n",
    "    drop3a  = Dropout(0.50)(dense3a)\n",
    "    \n",
    "    dense3 = Dense(nb_classes, activation='softmax')(drop3a)\n",
    "    model  = Model(inp, dense3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "              ,metrics=['accuracy'])\n",
    "    \n",
    "    # keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    # keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator_alenet3():\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.models import Model\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "    inp =  Input(shape=(frame_size,1))\n",
    "    #----------------------\n",
    "    conv1 = Conv1D(filters=16, kernel_size=128, strides=2, activation='relu')(inp)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    # 8 -> 4\n",
    "    pool1 = MaxPool1D(pool_size=4, strides=4)(norm1)\n",
    "    #----------------------\n",
    "    conv2 = Conv1D(filters=8, kernel_size=256, strides=2, activation='relu')(pool1)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    # 8 -> 4\n",
    "    pool2 = MaxPool1D(pool_size=4, strides=4)(norm2)\n",
    "    #----------------------\n",
    "    conv3 = Conv1D(filters=32, kernel_size=64, strides=2, activation='relu')(pool2)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    #----------------------\n",
    "    conv4 = Conv1D(filters=64, kernel_size=32, strides=2, activation='relu')(norm3)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    #----------------------\n",
    "    conv5 = Conv1D(filters=128, kernel_size=16, strides=2, activation='relu')(norm4)\n",
    "    norm5 = BatchNormalization()(conv5)\n",
    "    pool3 = MaxPool1D(pool_size=2, strides=2)(norm5)\n",
    "\n",
    "    flat = Flatten()(pool3)\n",
    "\n",
    "    #dense1 = Dense(1024, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "    \n",
    "    #dense1 = Dense(2048, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "\n",
    "    dense2 = Dense(1024, activation='relu')(flat)\n",
    "    drop2  = Dropout(0.60)(dense2)\n",
    "\n",
    "    dense2a = Dense(512, activation='relu')(drop2)\n",
    "    drop2a  = Dropout(0.50)(dense2a)\n",
    "    \n",
    "    dense3a = Dense(256, activation='relu')(drop2a)\n",
    "    drop3a  = Dropout(0.40)(dense3a)\n",
    "    \n",
    "    dense3 = Dense(nb_classes, activation='softmax')(drop3a)\n",
    "    model  = Model(inp, dense3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "              ,metrics=['accuracy'])\n",
    "    \n",
    "    # keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    # keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
    "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "\n",
    "def se_fn(x, amplifying_ratio):\n",
    "  num_features = x.shape[-1].value\n",
    "  x = GlobalAvgPool1D()(x)\n",
    "  x = Reshape((1, num_features))(x)\n",
    "  x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
    "  x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
    "  return x\n",
    "\n",
    "def basic_block(x, num_features, weight_decay, _):\n",
    "  x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
    "             kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPool1D(pool_size=3)(x)\n",
    "  return x\n",
    "\n",
    "def se_block(x, num_features, weight_decay, amplifying_ratio):\n",
    "  x = basic_block(x, num_features, weight_decay, amplifying_ratio)\n",
    "  x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
    "  return x\n",
    "\n",
    "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
    "  if num_features != x.shape[-1].value:\n",
    "    shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
    "                      kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "  else:\n",
    "    shortcut = x\n",
    "  x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
    "             kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "  x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
    "             kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  if amplifying_ratio > 0:\n",
    "    x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
    "  x = Add()([shortcut, x])\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPool1D(pool_size=3)(x)\n",
    "  return x\n",
    "\n",
    "def resemul(x, block_type='se', multi=True, init_features=128, amplifying_ratio=16,\n",
    "            drop_rate=0.5, weight_decay=0., num_classes=10):\n",
    "  \"\"\"Build a SampleCNN model.\n",
    "  Args:\n",
    "    batch_shape: A tensor shape including batch size (e.g. [23, 59049])\n",
    "    block_type: A type of convolutional block among {se|rese|res|basic}\n",
    "    multi: Whether to use multi-level feature aggregation.\n",
    "    init_features: Number of feature maps of the first convolution.\n",
    "    amplifying_ratio: Amplifying ratio of SE (not used for res and basic).\n",
    "    weight_decay: L2 weight decay rate.\n",
    "    drop_rate: Dropout rate.\n",
    "    num_classes: Number of classes to predict.\n",
    "  Returns:\n",
    "    Keras Model.\n",
    "  \"\"\"\n",
    "  if block_type == 'se':\n",
    "    block = se_block\n",
    "  elif block_type == 'rese':\n",
    "    block = rese_block\n",
    "  elif block_type == 'res':\n",
    "    block = rese_block\n",
    "    amplifying_ratio = -1\n",
    "  elif block_type == 'basic':\n",
    "    block = basic_block\n",
    "  else:\n",
    "    raise Exception('Unknown block type: ' + block_type)\n",
    "\n",
    "  # x = Input(tensor=x)\n",
    "  x = Input(shape=(frame_size,1))\n",
    "  x = Reshape([-1, 1])(x)\n",
    "\n",
    "  x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
    "             kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "\n",
    "  num_features = init_features\n",
    "  layer_outputs = []\n",
    "  for i in range(9):\n",
    "    num_features *= 2 if (i == 2 or i == 8) else 1\n",
    "    x = block(x, num_features, weight_decay, amplifying_ratio)\n",
    "    layer_outputs.append(x)\n",
    "\n",
    "  if multi:\n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
    "  else:\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "\n",
    "  x = Dense(x.shape[-1].value, kernel_initializer='glorot_uniform')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "\n",
    "  if drop_rate > 0.:\n",
    "    x = Dropout(drop_rate)(x)\n",
    "  x = Dense(num_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
    "  return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 110250, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 55062, 16)         2064      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 55062, 16)         64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 13765, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 6755, 8)           32776     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6755, 8)           32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 1688, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 813, 32)           16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 813, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 391, 64)           65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 391, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 188, 128)          131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 188, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 94, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12032)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              12321792  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 13,229,538\n",
      "Trainable params: 13,229,042\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "weights/weights_GTzan_fold2-10_110250_75_256_pool_alenet3.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "fold = 2-10\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "hist = []\n",
    "\n",
    "\n",
    "now = time.strftime(\"%c\")\n",
    "tbcallback = TensorBoard(log_dir='./tmp/GTzan_fold2-10_110250_75_256_'+now, histogram_freq=0, write_graph=True, write_images=True )\n",
    "\n",
    "#for train_index, test_index in kf.split(X):\n",
    "#generating the model \n",
    "model = model_generator_alenet3()\n",
    "\n",
    "\n",
    "# Create a model.\n",
    "  out = resemul(x_train, block_type=args.block, multi=not args.no_multi,\n",
    "                amplifying_ratio=args.alpha, drop_rate=args.dropout, weight_decay=args.weight_decay)\n",
    "  model = TFRecordModel(inputs=x_train, val_inputs=x_val, outputs=out)\n",
    "\n",
    "\n",
    "\n",
    "#checkpoints\n",
    "str0=\"weights/\"\n",
    "str1=\"weights_GTzan_fold2-10_110250_75_256_pool_resnet\" \n",
    "str2=\".best.hdf5\" \n",
    "filepath=str0+str1+str2 \n",
    "print(filepath)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') \n",
    "callbacks_list = [checkpoint, tbcallback]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22490 samples, validate on 2500 samples\n",
      "Epoch 1/300\n",
      "22490/22490 [==============================] - 268s 12ms/step - loss: 2.6729 - acc: 0.1774 - val_loss: 2.1047 - val_acc: 0.1832\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.18320, saving model to weights/weights_GTzan_fold2-10_110250_75_256_pool_alenet3.best.hdf5\n",
      "Epoch 2/300\n",
      "22490/22490 [==============================] - 268s 12ms/step - loss: 2.0234 - acc: 0.2724 - val_loss: 2.1022 - val_acc: 0.2408\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.18320 to 0.24080, saving model to weights/weights_GTzan_fold2-10_110250_75_256_pool_alenet3.best.hdf5\n",
      "Epoch 3/300\n",
      "22490/22490 [==============================] - 216s 10ms/step - loss: 1.8310 - acc: 0.3402 - val_loss: 2.1289 - val_acc: 0.2412\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.24080 to 0.24120, saving model to weights/weights_GTzan_fold2-10_110250_75_256_pool_alenet3.best.hdf5\n",
      "Epoch 4/300\n",
      "22490/22490 [==============================] - 266s 12ms/step - loss: 1.6899 - acc: 0.4039 - val_loss: 2.8857 - val_acc: 0.2556\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.24120 to 0.25560, saving model to weights/weights_GTzan_fold2-10_110250_75_256_pool_alenet3.best.hdf5\n",
      "Epoch 5/300\n",
      "22490/22490 [==============================] - 268s 12ms/step - loss: 1.5826 - acc: 0.4447 - val_loss: 1.8860 - val_acc: 0.3396\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.25560 to 0.33960, saving model to weights/weights_GTzan_fold2-10_110250_75_256_pool_alenet3.best.hdf5\n",
      "Epoch 6/300\n",
      "22490/22490 [==============================] - 217s 10ms/step - loss: 1.4874 - acc: 0.4889 - val_loss: 2.3667 - val_acc: 0.3268\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.33960\n",
      "Epoch 7/300\n",
      "22490/22490 [==============================] - 265s 12ms/step - loss: 1.3883 - acc: 0.5303 - val_loss: 2.1325 - val_acc: 0.3312\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.33960\n",
      "Epoch 8/300\n",
      "22490/22490 [==============================] - 265s 12ms/step - loss: 1.3042 - acc: 0.5637 - val_loss: 2.4557 - val_acc: 0.3280\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.33960\n",
      "Epoch 9/300\n",
      "22490/22490 [==============================] - 218s 10ms/step - loss: 1.2421 - acc: 0.5964 - val_loss: 2.0836 - val_acc: 0.3900\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.33960 to 0.39000, saving model to weights/weights_GTzan_fold2-10_110250_75_256_pool_alenet3.best.hdf5\n",
      "Epoch 10/300\n",
      "22490/22490 [==============================] - 261s 12ms/step - loss: 1.1717 - acc: 0.6239 - val_loss: 2.4138 - val_acc: 0.3412\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.39000\n",
      "Epoch 11/300\n",
      "22490/22490 [==============================] - 268s 12ms/step - loss: 1.1343 - acc: 0.6433 - val_loss: 2.0622 - val_acc: 0.3508\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.39000\n",
      "Epoch 12/300\n",
      "22490/22490 [==============================] - 248s 11ms/step - loss: 1.0756 - acc: 0.6624 - val_loss: 2.4439 - val_acc: 0.3392\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.39000\n",
      "Epoch 13/300\n",
      "22490/22490 [==============================] - 229s 10ms/step - loss: 1.0425 - acc: 0.6783 - val_loss: 1.9178 - val_acc: 0.3644\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.39000\n",
      "Epoch 14/300\n",
      "22490/22490 [==============================] - 268s 12ms/step - loss: 1.0207 - acc: 0.6863 - val_loss: 1.9894 - val_acc: 0.3868\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.39000\n",
      "Epoch 15/300\n",
      "22490/22490 [==============================] - 265s 12ms/step - loss: 0.9903 - acc: 0.6976 - val_loss: 1.9715 - val_acc: 0.3560\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.39000\n",
      "Epoch 16/300\n",
      " 7500/22490 [=========>....................] - ETA: 2:03 - loss: 0.9465 - acc: 0.7185"
     ]
    }
   ],
   "source": [
    "#fitting the model \n",
    "hist.append(model.fit(X_train, Y_train,\n",
    "                      batch_size = batch_size, \n",
    "                      epochs = nb_epoch,\n",
    "                      verbose = 1,\n",
    "                      shuffle = True,\n",
    "                      callbacks = callbacks_list,\n",
    "                      validation_data = (X_valid, Y_valid)\n",
    "                     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.music-ir.org/nema_out/mirex2017/results/act/latin_report/accperfold.html\n",
    "\n",
    "MIREX 2017:\n",
    "\n",
    "\n",
    "\n",
    "Summary Results    [top]\n",
    "Algorithm\n",
    "Classification Accuracy\n",
    "Normalised Classification Accuracy\n",
    "LPNKK1\n",
    "0.7586\n",
    "0.7571\n",
    "PLNPH1\n",
    "0.6619\n",
    "0.6573\n",
    "XLJ1\n",
    "0.6148\n",
    "0.6079\n",
    "LPNKK3\n",
    "0.7347\n",
    "0.7324\n",
    "LPNKK2\n",
    "0.6511\n",
    "0.6458\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Fold\n",
    "LPNKK1\n",
    "LPNKK2\n",
    "LPNKK3\n",
    "PLNPH1\n",
    "XLJ1\n",
    "1\n",
    "0.7186\n",
    "0.6041\n",
    "0.6860\n",
    "0.5795\n",
    "0.5453\n",
    "0\n",
    "0.7627\n",
    "0.6426\n",
    "0.7444\n",
    "0.6772\n",
    "0.5998\n",
    "2\n",
    "0.8055\n",
    "0.7194\n",
    "0.7872\n",
    "0.7518\n",
    "0.7183\n",
    "\n",
    "\n",
    "/Users/akoerich/Dropbox/Mendeley/pdf/Aytar, Vondrick, Torralba - 2016.pdf\n",
    "\n",
    "/Users/akoerich/HOME/ETS/Etudiants/Sajjad Abdoli/DGA1032/DGA1032-Dissertation_abdoli.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
