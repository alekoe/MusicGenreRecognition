{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "# End to end CNN for GTzan music classification EnvCNN\n",
    "\n",
    "\n",
    "WINDOWED Version\n",
    "\n",
    "Adapted by AL Koerich\n",
    "\n",
    "To GTzan 3-fold\n",
    "\n",
    "11 December 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import soundfile as sf\n",
    "from numpy import *\n",
    "import soundfile as sf\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "from keras import regularizers\n",
    "\n",
    "import os, sys\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, Dense, MaxPool1D, Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "import keras.initializers as init\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( )\n",
    "config.gpu_options.allow_growth = True\n",
    "sess   = tf.Session(config=config)\n",
    "import keras.backend.tensorflow_backend as tf_bkend\n",
    "tf_bkend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#controling_Hyper parameters\n",
    "batch_size = 100\n",
    "nb_classes = 10\n",
    "nb_epoch   = 300\n",
    "frame_size = 110250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load( \"/home-2/akoerich/GTzan_Xs_fold1-3_110250_75.npy\" )\n",
    "Y_train = np.load( \"/home-2/akoerich/GTzan_Ys_fold1-3_110250_75.npy\" )\n",
    "# t_train = np.load( \"t_train_13_220500_50_256_box_id.npy\" )\n",
    "# s_train = np.load( \"s_train_13_220500_50_256_box_id.npy\" )\n",
    "\n",
    "\n",
    "X_valid = np.load( \"/home-2/akoerich/GTzan_Xs_fold2_110250_75.npy\" )\n",
    "Y_valid = np.load( \"/home-2/akoerich/GTzan_Ys_fold2_110250_75.npy\" )\n",
    "# t_valid = np.load( \"t_valid_2_220500_50_256_box_id.npy\" )\n",
    "# s_valid = np.load( \"s_valid_2_220500_50_256_box_id.npy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.min(), X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.min(), X_valid.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a validation split is not provided\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_valid, Y_train, Y_valid = train_test_split(X_train2, Y_train2, test_size=0.10, stratify=Y_train2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#X_train2 = None\n",
    "#Y_train2 = None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape\n",
    "#, t_train.shape, s_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_generator_soundnet():\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.models import Model\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "    inp =  Input(shape=(frame_size,1))\n",
    "    #----------------------\n",
    "    conv1 = Conv1D(filters=8, kernel_size=256, strides=2, activation='relu')(inp)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPool1D(pool_size=8, strides=8)(norm1)\n",
    "    #----------------------\n",
    "    conv2 = Conv1D(filters=16, kernel_size=128, strides=2, activation='relu')(pool1)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPool1D(pool_size=8, strides=8)(norm2)\n",
    "    #----------------------\n",
    "    conv3 = Conv1D(filters=32, kernel_size=64, strides=2, activation='relu')(pool2)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    #----------------------\n",
    "    conv4 = Conv1D(filters=64, kernel_size=32, strides=2, activation='relu')(norm3)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    #----------------------\n",
    "    conv5 = Conv1D(filters=128, kernel_size=16, strides=2, activation='relu')(norm4)\n",
    "    norm5 = BatchNormalization()(conv5)\n",
    "    pool3 = MaxPool1D(pool_size=4, strides=4)(norm5)\n",
    "\n",
    "    flat = Flatten()(pool3)\n",
    "\n",
    "    dense1 = Dense(1024, activation='relu')(flat)\n",
    "    drop1  = Dropout(0.50)(dense1)\n",
    "    \n",
    "    # dense1 = Dense(2048, activation='relu')(flat)\n",
    "    # drop1  = Dropout(0.50)(dense1)\n",
    "\n",
    "    # dense2 = Dense(1024, activation='relu')(dense1)\n",
    "    # drop2  = Dropout(0.50)(dense2)\n",
    "\n",
    "    dense2a = Dense(512, activation='relu')(drop1)\n",
    "    drop2a  = Dropout(0.50)(dense2a)\n",
    "    \n",
    "    dense3a = Dense(256, activation='relu')(drop2a)\n",
    "    drop3a  = Dropout(0.50)(dense3a)\n",
    "    \n",
    "    dense3 = Dense(nb_classes, activation='softmax')(drop3a)\n",
    "    model  = Model(inp, dense3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "              ,metrics=['accuracy'])\n",
    "    \n",
    "    # keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    # keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator_soundnet2():\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.models import Model\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "    inp =  Input(shape=(frame_size,1))\n",
    "    #----------------------\n",
    "    conv1 = Conv1D(filters=16, kernel_size=128, strides=2, activation='relu')(inp)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPool1D(pool_size=8, strides=8)(norm1)\n",
    "    #----------------------\n",
    "    conv2 = Conv1D(filters=32, kernel_size=64, strides=2, activation='relu')(pool1)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPool1D(pool_size=8, strides=8)(norm2)\n",
    "    #----------------------\n",
    "    conv3 = Conv1D(filters=64, kernel_size=32, strides=2, activation='relu')(pool2)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    #----------------------\n",
    "    conv4 = Conv1D(filters=32, kernel_size=16, strides=2, activation='relu')(norm3)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    #----------------------\n",
    "    #conv5 = Conv1D(filters=16, kernel_size=8, strides=2, activation='relu')(norm4)\n",
    "    #norm5 = BatchNormalization()(conv5)\n",
    "    #pool3 = MaxPool1D(pool_size=4, strides=4)(norm5)\n",
    "\n",
    "    pool3 = MaxPool1D(pool_size=4, strides=4)(norm4)\n",
    "\n",
    "    \n",
    "    flat = Flatten()(pool3)\n",
    "\n",
    "    dense1 = Dense(2048, activation='relu')(flat)\n",
    "    drop1  = Dropout(0.50)(dense1)\n",
    "\n",
    "    dense2 = Dense(1024, activation='relu')(drop1)\n",
    "    drop2  = Dropout(0.50)(dense2)\n",
    "\n",
    "    dense2a = Dense(512, activation='relu')(drop2)\n",
    "    drop2a  = Dropout(0.50)(dense2a)\n",
    "    \n",
    "    dense3a = Dense(256, activation='relu')(drop2a)\n",
    "    drop3a  = Dropout(0.50)(dense3a)\n",
    "    \n",
    "    dense3 = Dense(nb_classes, activation='softmax')(drop3a)\n",
    "    model  = Model(inp, dense3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "              ,metrics=['accuracy'])\n",
    "    \n",
    "    # keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    # keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator_soundnet3():\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.models import Model\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "    inp =  Input(shape=(frame_size,1))\n",
    "    #----------------------\n",
    "    conv1 = Conv1D(filters=8, kernel_size=256, strides=2, activation='relu')(inp)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPool1D(pool_size=8, strides=8)(norm1)\n",
    "    #----------------------\n",
    "    conv2 = Conv1D(filters=16, kernel_size=128, strides=2, activation='relu')(pool1)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPool1D(pool_size=8, strides=8)(norm2)\n",
    "    #----------------------\n",
    "    conv3 = Conv1D(filters=32, kernel_size=64, strides=2, activation='relu')(pool2)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    #----------------------\n",
    "    conv4 = Conv1D(filters=64, kernel_size=32, strides=2, activation='relu')(norm3)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    #----------------------\n",
    "    conv5 = Conv1D(filters=128, kernel_size=16, strides=2, activation='relu')(norm4)\n",
    "    norm5 = BatchNormalization()(conv5)\n",
    "    pool3 = MaxPool1D(pool_size=4, strides=4)(norm5)\n",
    "\n",
    "    flat = Flatten()(pool3)\n",
    "\n",
    "    #dense1 = Dense(2048, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "\n",
    "    dense2 = Dense(1024, activation='relu')(flat)\n",
    "    drop2  = Dropout(0.50)(dense2)\n",
    "\n",
    "    dense2a = Dense(512, activation='relu')(drop2)\n",
    "    drop2a  = Dropout(0.50)(dense2a)\n",
    "    \n",
    "    dense3a = Dense(256, activation='relu')(drop2a)\n",
    "    drop3a  = Dropout(0.50)(dense3a)\n",
    "    \n",
    "    dense3 = Dense(nb_classes, activation='softmax')(drop3a)\n",
    "    model  = Model(inp, dense3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "              ,metrics=['accuracy'])\n",
    "    \n",
    "    # keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    # keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator_alenet():\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.models import Model\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "    inp =  Input(shape=(frame_size,1))\n",
    "    #----------------------\n",
    "    conv1 = Conv1D(filters=8, kernel_size=256, strides=2, activation='relu')(inp)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    # 8 -> 4\n",
    "    pool1 = MaxPool1D(pool_size=4, strides=4)(norm1)\n",
    "    #----------------------\n",
    "    conv2 = Conv1D(filters=16, kernel_size=128, strides=2, activation='relu')(pool1)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    # 8 -> 4\n",
    "    pool2 = MaxPool1D(pool_size=4, strides=4)(norm2)\n",
    "    #----------------------\n",
    "    conv3 = Conv1D(filters=32, kernel_size=64, strides=2, activation='relu')(pool2)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    #----------------------\n",
    "    conv4 = Conv1D(filters=64, kernel_size=32, strides=2, activation='relu')(norm3)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    #----------------------\n",
    "    conv5 = Conv1D(filters=128, kernel_size=16, strides=2, activation='relu')(norm4)\n",
    "    norm5 = BatchNormalization()(conv5)\n",
    "    pool3 = MaxPool1D(pool_size=2, strides=2)(norm5)\n",
    "\n",
    "    flat = Flatten()(pool3)\n",
    "\n",
    "    #dense1 = Dense(1024, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "    \n",
    "    #dense1 = Dense(2048, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "\n",
    "    dense2 = Dense(1024, activation='relu')(flat)\n",
    "    drop2  = Dropout(0.50)(dense2)\n",
    "\n",
    "    dense2a = Dense(512, activation='relu')(drop2)\n",
    "    drop2a  = Dropout(0.50)(dense2a)\n",
    "    \n",
    "    dense3a = Dense(256, activation='relu')(drop2a)\n",
    "    drop3a  = Dropout(0.50)(dense3a)\n",
    "    \n",
    "    dense3 = Dense(nb_classes, activation='softmax')(drop3a)\n",
    "    model  = Model(inp, dense3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "              ,metrics=['accuracy'])\n",
    "    \n",
    "    # keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    # keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best so far.\n",
    "# Epoch 00300: val_acc did not improve from 0.50920\n",
    "\n",
    "def model_generator_alenet2():\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.models import Model\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "    inp =  Input(shape=(frame_size,1))\n",
    "    #----------------------\n",
    "    conv1 = Conv1D(filters=16, kernel_size=128, strides=2, activation='relu')(inp)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    # 8 -> 4\n",
    "    pool1 = MaxPool1D(pool_size=4, strides=4)(norm1)\n",
    "    #----------------------\n",
    "    conv2 = Conv1D(filters=8, kernel_size=256, strides=2, activation='relu')(pool1)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    # 8 -> 4\n",
    "    pool2 = MaxPool1D(pool_size=4, strides=4)(norm2)\n",
    "    #----------------------\n",
    "    conv3 = Conv1D(filters=32, kernel_size=64, strides=2, activation='relu')(pool2)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    #----------------------\n",
    "    conv4 = Conv1D(filters=64, kernel_size=32, strides=2, activation='relu')(norm3)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    #----------------------\n",
    "    conv5 = Conv1D(filters=128, kernel_size=16, strides=2, activation='relu')(norm4)\n",
    "    norm5 = BatchNormalization()(conv5)\n",
    "    pool3 = MaxPool1D(pool_size=2, strides=2)(norm5)\n",
    "\n",
    "    flat = Flatten()(pool3)\n",
    "\n",
    "    #dense1 = Dense(1024, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "    \n",
    "    #dense1 = Dense(2048, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "\n",
    "    dense2 = Dense(1024, activation='relu')(flat)\n",
    "    drop2  = Dropout(0.50)(dense2)\n",
    "\n",
    "    dense2a = Dense(512, activation='relu')(drop2)\n",
    "    drop2a  = Dropout(0.50)(dense2a)\n",
    "    \n",
    "    dense3a = Dense(256, activation='relu')(drop2a)\n",
    "    drop3a  = Dropout(0.50)(dense3a)\n",
    "    \n",
    "    dense3 = Dense(nb_classes, activation='softmax')(drop3a)\n",
    "    model  = Model(inp, dense3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "              ,metrics=['accuracy'])\n",
    "    \n",
    "    # keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    # keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator_alenet3():\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.layers import AveragePooling1D\n",
    "    from keras.models import Model\n",
    "    from keras import initializers\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "    inp =  Input(shape=(frame_size,1))\n",
    "    #----------------------\n",
    "    conv1 = Conv1D(filters=32, kernel_size=128, strides=2, activation='relu')(inp)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    # 8 -> 4\n",
    "    pool1 = AveragePooling1D(pool_size=4, strides=4)(norm1)\n",
    "    #----------------------\n",
    "    conv2 = Conv1D(filters=16, kernel_size=256, strides=2, activation='relu')(pool1)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    # 8 -> 4\n",
    "    pool2 = AveragePooling1D(pool_size=4, strides=4)(norm2)\n",
    "    #----------------------\n",
    "    conv3 = Conv1D(filters=32, kernel_size=64, strides=2, activation='relu')(pool2)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    #----------------------\n",
    "    conv4 = Conv1D(filters=64, kernel_size=32, strides=2, activation='relu')(norm3)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    #----------------------\n",
    "    conv5 = Conv1D(filters=128, kernel_size=16, strides=2, activation='relu')(norm4)\n",
    "    norm5 = BatchNormalization()(conv5)\n",
    "    pool3 = AveragePooling1D(pool_size=2, strides=2)(norm5)\n",
    "\n",
    "    flat = Flatten()(pool3)\n",
    "\n",
    "    #dense1 = Dense(1024, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "    \n",
    "    #dense1 = Dense(2048, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "\n",
    "    dense2 = Dense(1024, activation='relu', kernel_initializer=initializers.glorot_uniform(seed=0) )(flat)\n",
    "    drop2  = Dropout(0.55)(dense2)\n",
    "\n",
    "    dense2a = Dense(512, activation='relu', kernel_initializer=initializers.glorot_uniform(seed=0) )(drop2)\n",
    "    drop2a  = Dropout(0.50)(dense2a)\n",
    "    \n",
    "    dense3a = Dense(256, activation='relu', kernel_initializer=initializers.glorot_uniform(seed=0) )(drop2a)\n",
    "    drop3a  = Dropout(0.50)(dense3a)\n",
    "    \n",
    "    dense3 = Dense(nb_classes, activation='softmax')(drop3a)\n",
    "    model  = Model(inp, dense3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "              ,metrics=['accuracy'])\n",
    "    \n",
    "    # keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    # keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best so far 0.53160 for 10-fold\n",
    "# dec 10. comecou dec 9 tarde.\n",
    "\n",
    "\n",
    "# best so far 0.64008 for 3-fold\n",
    "# GTzan_3f_fold1-2_110250_75Tue Dec 11 10:26:42 2018\n",
    "\n",
    "\n",
    "def model_generator_alenet4():\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.layers import AveragePooling1D\n",
    "    from keras.models import Model\n",
    "    from keras import initializers\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from keras.layers import LeakyReLU    \n",
    "    \n",
    "    inp =  Input(shape=(frame_size,1))\n",
    "    #----------------------\n",
    "    conv1 = Conv1D(filters=32, kernel_size=128, strides=2)(inp)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    act1  = LeakyReLU(alpha=0.1)(norm1)\n",
    "    drop1 = Dropout(0.10)(act1)\n",
    "    # 8 -> 4\n",
    "    pool1 = AveragePooling1D(pool_size=4, strides=4)(drop1)\n",
    "    #----------------------\n",
    "    conv2 = Conv1D(filters=16, kernel_size=256, strides=2)(pool1)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    act2  = LeakyReLU(alpha=0.1)(norm2)\n",
    "    drop2b = Dropout(0.10)(act2)\n",
    "    # 8 -> 4\n",
    "    pool2 = AveragePooling1D(pool_size=4, strides=4)(drop2b)\n",
    "    #----------------------\n",
    "    conv3 = Conv1D(filters=32, kernel_size=64, strides=2 )(pool2)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    act3  = LeakyReLU(alpha=0.1)(norm3)\n",
    "    drop3b = Dropout(0.10)(act3)\n",
    "    #----------------------\n",
    "    conv4 = Conv1D(filters=64, kernel_size=32, strides=2 )(drop3b)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    act4  = LeakyReLU(alpha=0.1)(norm4)\n",
    "    drop4b = Dropout(0.10)(act4)\n",
    "    #----------------------\n",
    "    conv5 = Conv1D(filters=128, kernel_size=16, strides=2 )(drop4b)\n",
    "    norm5 = BatchNormalization()(conv5)\n",
    "    act5  = LeakyReLU(alpha=0.1)(norm5)\n",
    "    drop5b = Dropout(0.10)(act5)\n",
    "    \n",
    "    pool3 = AveragePooling1D(pool_size=2, strides=2)(drop5b)\n",
    "\n",
    "    flat = Flatten()(pool3)\n",
    "\n",
    "    #dense1 = Dense(1024, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "    \n",
    "    #dense1 = Dense(2048, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "\n",
    "    dense2 = Dense(1024, activation='relu', kernel_initializer=initializers.glorot_uniform(seed=0) )(flat)\n",
    "    drop2  = Dropout(0.55)(dense2)\n",
    "\n",
    "    dense2a = Dense(512, activation='relu', kernel_initializer=initializers.glorot_uniform(seed=0) )(drop2)\n",
    "    drop2a  = Dropout(0.55)(dense2a)\n",
    "    \n",
    "    dense3a = Dense(256, activation='relu', kernel_initializer=initializers.glorot_uniform(seed=0) )(drop2a)\n",
    "    drop3a  = Dropout(0.50)(dense3a)\n",
    "    \n",
    "    dense3 = Dense(nb_classes, activation='softmax')(drop3a)\n",
    "    model  = Model(inp, dense3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "              ,metrics=['accuracy'])\n",
    "    \n",
    "    # keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    # keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator_alenet5():\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.layers import AveragePooling1D\n",
    "    from keras.models import Model\n",
    "    from keras import initializers\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from keras.layers import LeakyReLU    \n",
    "    \n",
    "    inp =  Input(shape=(frame_size,1))\n",
    "    #----------------------\n",
    "    conv1 = Conv1D(filters=32, kernel_size=128, strides=2)(inp)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    act1  = LeakyReLU(alpha=0.1)(norm1)\n",
    "    drop1 = Dropout(0.10)(act1)\n",
    "    # 8 -> 4\n",
    "    pool1 = AveragePooling1D(pool_size=4, strides=4)(drop1)\n",
    "    #----------------------\n",
    "    conv2 = Conv1D(filters=16, kernel_size=256, strides=2)(pool1)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    act2  = LeakyReLU(alpha=0.1)(norm2)\n",
    "    drop2b = Dropout(0.10)(act2)\n",
    "    # 8 -> 4\n",
    "    pool2 = AveragePooling1D(pool_size=4, strides=4)(drop2b)\n",
    "    #----------------------\n",
    "    conv3 = Conv1D(filters=32, kernel_size=64, strides=2 )(pool2)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    act3  = LeakyReLU(alpha=0.1)(norm3)\n",
    "    drop3b = Dropout(0.10)(act3)\n",
    "    #----------------------\n",
    "    conv4 = Conv1D(filters=64, kernel_size=32, strides=2 )(drop3b)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    act4  = LeakyReLU(alpha=0.1)(norm4)\n",
    "    drop4b = Dropout(0.10)(act4)\n",
    "    #----------------------\n",
    "    conv5 = Conv1D(filters=128, kernel_size=16, strides=2 )(drop4b)\n",
    "    norm5 = BatchNormalization()(conv5)\n",
    "    act5  = LeakyReLU(alpha=0.1)(norm5)\n",
    "    drop5b = Dropout(0.10)(act5)\n",
    "    \n",
    "    #pool3 = AveragePooling1D(pool_size=2, strides=2)(drop5b)\n",
    "\n",
    "    flat = Flatten()(drop5b)\n",
    "\n",
    "    #dense1 = Dense(1024, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "    \n",
    "    #dense1 = Dense(2048, activation='relu')(flat)\n",
    "    #drop1  = Dropout(0.50)(dense1)\n",
    "\n",
    "    dense2 = Dense(1024, kernel_initializer=initializers.glorot_uniform(seed=0) )(flat)\n",
    "    actd2  = LeakyReLU(alpha=0.1)(dense2)\n",
    "    drop2  = Dropout(0.6)(actd2)\n",
    "\n",
    "    dense2a = Dense(512, kernel_initializer=initializers.glorot_uniform(seed=0) )(drop2)\n",
    "    actd2a  = LeakyReLU(alpha=0.1)(dense2a)\n",
    "    drop2a  = Dropout(0.5)(actd2a)\n",
    "    \n",
    "    #dense3a = Dense(256, activation='relu', kernel_initializer=initializers.glorot_uniform(seed=0) )(drop2a)\n",
    "    #drop3a  = Dropout(0.50)(dense3a)\n",
    "    \n",
    "    dense3 = Dense(nb_classes, activation='softmax')(drop2a)\n",
    "    model  = Model(inp, dense3)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "              ,metrics=['accuracy'])\n",
    "    \n",
    "    # keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    # keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fold = 1-3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "hist = []\n",
    "\n",
    "\n",
    "now = time.strftime(\"%c\")\n",
    "tbcallback = TensorBoard(log_dir='./tmp/GTzan_3f_fold1-3_110250_75_suffle_'+now, histogram_freq=0, write_graph=True, write_images=True )\n",
    "\n",
    "#for train_index, test_index in kf.split(X):\n",
    "#generating the model \n",
    "model = model_generator_alenet4()\n",
    "\n",
    "#checkpoints\n",
    "str0=\"weights/\"\n",
    "str1=\"weights_GTzan_3f_fold1-3_110250_75_shuffle_alenet4\" \n",
    "str2=\".best.hdf5\" \n",
    "filepath=str0+str1+str2 \n",
    "print(filepath)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') \n",
    "callbacks_list = [checkpoint, tbcallback]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fitting the model \n",
    "hist.append(model.fit(X_train, Y_train,\n",
    "                      batch_size = batch_size, \n",
    "                      epochs = nb_epoch,\n",
    "                      verbose = 1,\n",
    "                      shuffle = True,\n",
    "                      callbacks = callbacks_list,\n",
    "                      validation_data = (X_valid, Y_valid)\n",
    "                     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.music-ir.org/nema_out/mirex2017/results/act/latin_report/accperfold.html\n",
    "\n",
    "MIREX 2017:\n",
    "\n",
    "\n",
    "\n",
    "Summary Results    [top]\n",
    "Algorithm\n",
    "Classification Accuracy\n",
    "Normalised Classification Accuracy\n",
    "LPNKK1\n",
    "0.7586\n",
    "0.7571\n",
    "PLNPH1\n",
    "0.6619\n",
    "0.6573\n",
    "XLJ1\n",
    "0.6148\n",
    "0.6079\n",
    "LPNKK3\n",
    "0.7347\n",
    "0.7324\n",
    "LPNKK2\n",
    "0.6511\n",
    "0.6458\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Fold\n",
    "LPNKK1\n",
    "LPNKK2\n",
    "LPNKK3\n",
    "PLNPH1\n",
    "XLJ1\n",
    "1\n",
    "0.7186\n",
    "0.6041\n",
    "0.6860\n",
    "0.5795\n",
    "0.5453\n",
    "0\n",
    "0.7627\n",
    "0.6426\n",
    "0.7444\n",
    "0.6772\n",
    "0.5998\n",
    "2\n",
    "0.8055\n",
    "0.7194\n",
    "0.7872\n",
    "0.7518\n",
    "0.7183\n",
    "\n",
    "\n",
    "/Users/akoerich/Dropbox/Mendeley/pdf/Aytar, Vondrick, Torralba - 2016.pdf\n",
    "\n",
    "/Users/akoerich/HOME/ETS/Etudiants/Sajjad Abdoli/DGA1032/DGA1032-Dissertation_abdoli.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
